% Emacs, this is -*-latex-*-

\title{\href{https://github.com/Sistemas-Multimedia/PRMC}{Progressive Resolution Motion Compensation (PRMC)}}

\author{Vicente Gonz√°lez Ruiz}

%\makeglossaries

\maketitle

\tableofcontents

\section{Transform coding and compression}
%{{{

\href{https://en.wikipedia.org/wiki/Video}{Video} data contain high
amounts of \mylink{redundancy}{redundancy}, spatial and temporal. For
this reason, most of \mylink{video_compression}{video encoders}
compress an input sequence of
\href{https://en.wikipedia.org/wiki/Digital_image}{images}\footnote{Also
  called frames, which from a pure mathematical point of view are
  matrices of \href{https://en.wikipedia.org/wiki/Pixel}{pixels}.},
basically, in two stages: (1) a transform in the
\href{https://en.wikipedia.org/wiki/Time_domain}{time} and in the
\href{https://www.quora.com/What-is-spatial-domain-in-image-processing}{spatial}
domains that produces a collection of
\href{https://en.wikipedia.org/wiki/Decorrelation}{uncorrelated}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{coefficients},
and (2) an
\href{https://en.wikipedia.org/wiki/Entropy_encoding}{entropy
  encoding} stage which removes the statistical redundancy that can
still remains after the decorrelation (transform). Usually, these
coefficients have two interesting features:
\begin{enumerate}
\item Usually, a \textbf{
  \href{https://vicente-gonzalez-ruiz.github.io/symbol_compression/}{smaller
  entropy}} than the original pixels. This helps to increase
  the \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio}.
\item A
  \textbf{\href{https://en.wikipedia.org/wiki/Image_resolution}{multiresolution
      representation} (in space and time)} of the visual
  information. This provides the posibility of decoding the sequence
  of images using a smaller resolution (in space and time). This
  feature is known as \mylink{video_compression}{spatio-temporal
    scalability in video}.
\item Usually, \textbf{most of the
  \href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{signal
    energy}} (and therefore, generally, most of the information
  interesting for the humans) \textbf{is represented by
    \href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding}{a
      small number of coefficients}}. This helps to improve the
  compression ratio and to prioritize the data in
  \mylink{media_encoding_models}{progressive transmission scenarios}.
\end{enumerate}

%}}}

\section{The 2D-DWT (Distrete Wavelet Transform)}
%{{{

\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/#x1-3100020}{The
  2D-DWT (2-Dimensions Distrete Wavelet Transform)\footnote{In this
    document, we will use the term DWT to refeer to the 2D-DWT.}} is a
digital transform that, applied to an image, performs a spatial
decorrelation of the pixels and obtains a
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/index.html#x1-3500024}{multiresolution
  (generally dyadic) representation} of such image, conforming a
collection of spatial subbands or
\href{https://en.wikipedia.org/wiki/Sub-band_coding}{decomposition}\footnote{The
  structure that forms the subbands is called also a
  decomposition.}. The forward transform converts the image into a set
of 4 frequency subbands in three differemt orientations (horizontal,
vertical and oblique) with coefficients representing different spatial
areas, depending on localization of the coefficient and the resolution
level. Such subbands are typically denoted by $LL$, $LH$, $HL$ and
$HH$, were $L$ stands for 1D low-pass \href{https://en.wikipedia.org/wiki/Filter_bank}{analysis} \href{https://en.wikipedia.org/wiki/Filter_(signal_processing)}{filtering} and $H$ for
1D high-pass analysis filtering. Thus, for example, $LH$ is the result
of applying the $L$ filter to the rows and downsample\footnote{This
  operation is also called ``decimation'' and ``subsampling''.} the
output by a factor of 2, and then applying the $H$ filter to the
columns of the previous decomposition and again, donwsampling by a
factor of 2. When we can compute the (2D-)DWT using 1D filters we say
that the DWT is separable.

The downsampling operation generates a critically sampled structure,
where the number of output DWT coefficients is equal to the number of
input pixels. This is possible because ideally, the $L$ filter is
designed to reject the frecuencies that the $H$ filter pass, and
viceversa (it is said that the filters are complementary, or that they
form a
\href{https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banks}{perfect-reconstruction
  filter bank}). In the practice, there is always some aliasing
(overlaping between the response of the filters in the frequency
domain), but this does not affect\footnote{This affects to the
  idependency of the coefficients in terms of energy contribution to
  the reconstruction of the signal.} to the number of output
coefficients. In terms of
\href{https://en.wikipedia.org/wiki/Linear_algebra}{Linear Algebra},
analysis $L$ and $H$ filters, and the synthesis $L^{-1}$ and $H^{-1}$
filters (the filters that reconstruct the pixels from the coefficients
generated by $L$ and $H$) would be
\href{https://en.wikipedia.org/wiki/Orthogonality}{orthogonal}, that
is, the basis functions that they convolve with the signals are
completely indenpendent.

%}}}

\section{Multi-level DWT structures}
%{{{

A $N$-iterations\footnote{Notice that
  \href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}
  uses the term ``levels'' for refering to the number of iterations of
  the 1-levels DWT.} dyadic DWT is the result of a recursive use of
the 1-iteration DWT applied to the $LL$ subband, $N$ times, producing
a $N+1$-levels dyadic structure. Thus, for example, if a DWT has been
applied $k$ times (or $k$ iterations) to the $LL$ subband,
recursively, a decomposition of $(k+1)$ Spatial Resolution Levels
(SRL's) is generated. For example, in the Fig.~\ref{fig:2-levels_DWT}
the DWT has been applied 2-times, generating 3 SRLs ($LL_2$,
$LL_1=\mathtt{iDWT}(LL_2, LH_2, HL_2, HH_2)$, and
$LL_0=\mathtt{iDWT}(LL_1, LH_1, HL_1, HH_1)$, where $\mathtt{iDWT}$
stands for inverse DWT).

\begin{figure}
\begin{verbatim}
    +------+------+-------------+
    |      |      |             |
    | LL_2 | LH_2 |             |
    |      |      |             |
    +------+------+     LH_1    | 
    |      |      |             |
    | HL_2 | HH_2 |             |
    |      |      |             |
    +------+------+-------------+ Y
    |             |             | 
    |             |             |
    |             |             | 
    |     HL_1    |     HH_1    | 
    |             |             | 
    |             |             |
    |             |             |
    +-------------+-------------+
                  X
\end{verbatim}
\caption{Decomposition generated by $\mathtt{DWT}(\mathtt{iters}=2)$.}
\end{figure}

For the sake of
simplicity, we will denote the spatial image\footnote{``Image'' is the
  name that most video coding standards use for reffering to a single
  image of the video.}  subbands $\{F.LH_k, F.HL_k, F.HH_k\}$ as only
$F.H_k$, and $F.LL_k$ as only $F.L_k$, where $F$ is a image. See
\href{https://nbviewer.jupyter.org/github/Sistemas-Multimedia/PRMC/blob/master/docs/DWT_vs_LPT.ipynb}{DWT
  versus LPT}.

%}}}

\section{Scalability provided by the 2D-DWT}
%{{{

Depending on the number of subband levels (pyramid levels in the
\mylink{pyramids-and-wavelets}{pyramid representation}), we can
reconstruct a image
at different spatial scales (resolutions). For example, if a image
$F_1$ has been transformed using the DWT of 2 iterations, we get:
\begin{itemize}
\item Subband $F_1.L_2$ with the scale 2.
\begin{verbatim}
    +------+
    |      |
Y/4 |  L_2 |
    |      |
    +------+
       X/4
\end{verbatim}
\item Subband $F_1.L_1\coloneqq F_1.L=\text{iDWT}(F_1.L_2, F_1.H_2)$
  with the scale $1$.
\begin{verbatim}
    +-------------+
    |             |
    |             |
Y/2 |     L_1     |
    |             | 
    |             |
    +-------------+
          X/2
\end{verbatim}
  
\item $F_1.L_0=\text{iDWT}(F_1.L, F_1.H)$ (the original image) with
  the scale $0$.
\begin{verbatim}
    +---------------------------+
    |                           |
    |                           |
    |                           | 
    |                           |
    |                           |
    |                           |
  Y |            L_0            |
    |                           | 
    |                           |
    |                           | 
    |                           | 
    |                           |
    |                           |
    +---------------------------+
                  X
\end{verbatim}
\end{itemize}

%}}}

\section*{Implementation of the 3 components (color) DWT step}
%{{{

\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/DWT.py}{DWT.py}
(see Listing.~\ref{fig:DWT}) implements the forward (DWT) and backward
(iDWT) 1-iteration (step) 2D DWT for color images.

\begin{figure}
  \lstinputlisting[firstline=25, lastline=94, language=python, caption=1-iteration DWT
    (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/DWT.py}{DWT.py}).]{../src/DWT.py}
  \lstinputlisting[firstline=96, lastline=118, language=python,
    caption=1-iteration iDWT
    (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/DWT.py}{DWT.py}).]{../src/DWT.py}
  \caption{Implementation of the color DWT step (forward and
    backward). $L$ and $H$ stands for \emph{low-pass filtering} and
    \emph{high-pass filtering}, respectively.  More information about the
    implementation can be found at
    \href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}.}
  \label{fig:DWT}
\end{figure}

%}}}

%{{{ Examples

\subsection*{Example: A 1-iteration 2D-DWT (\texttt{DWT(iters=1)})}
%{{{

An image is transformed, generating four subbands $LL$, $LH$, $HL$ and
$HH$.
\begin{verbatim}
# You must be in the 'src' directory.

# Copy an image (the output directory must be the same as the input one).
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# 1-iteration 2D DWT.
python3 -O DWT.py -p /tmp/ -i 000

# Visualize the subbands.
display -normalize /tmp/LL000.png # <- LL_1
display -normalize /tmp/LH000.png # <- LH_1
display -normalize /tmp/HL000.png # <- HL_1
display -normalize /tmp/HH000.png # <- HH_1

# 1-iteration 2D iDWT.
python3 -O DWT.py -b -p /tmp/ -i 000

# Visualize the reconstruction.
python3 ../tools/substract_offset.py -i /tmp/000.png -o /tmp/1.png
display /tmp/1.png

# Show diferences (PyWavelets uses floating point arithmetic).
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display /tmp/diffs.png
\end{verbatim}

%}}}

\subsection*{Example: A 2-iterations 2D-DWT (\texttt{DWT(iters=2)})}
%{{{

\begin{verbatim}
# You must be in the 'src' directory.

# Copy a image (the output directory must be the same than the input one).
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# First iteration.
python3 -O DWT.py -p /tmp/ -i 000

# Second iteration.
python3 -O DWT.py -p /tmp/LL -i 000

# Visualize the subbands.
ls -l /tmp/*.png
display -normalize /tmp/LL000.png # <- LL_1
display -normalize /tmp/LH000.png # <- LH_1
display -normalize /tmp/HL000.png # <- HL_1
display -normalize /tmp/HH000.png # <- HH_1
display -normalize /tmp/LLLL000.png # <- LL_2
display -normalize /tmp/LLLH000.png # <- LH_2
display -normalize /tmp/LLHL000.png # <- HL_2
display -normalize /tmp/LLHH000.png # <- HH_2

# Reverse second iteration.
python3 -O DWT.py -b -p /tmp/LL -i 000
display -normalize /tmp/LL000.png

# Reverse first iteration.
python3 -O DWT.py -b -p /tmp/ -i 000
python3 ../tools/substract_offset.py -i /tmp/000.png -o /tmp/1.png
display /tmp/1.png

# Show diferences.
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display /tmp/diffs.png
\end{verbatim}

%}}}

%}}}

\section{Motion DWT (MDWT)}
%{{{

The 2D-DWT can be applied to a sequence of images by simply
transforming each image of the sequence independently. This is done,
for example, in the \mylink{JPEG2000}{Motion JPEG2000 video
  compression standard}. Notice that only the spatial redundancy is
exploited in MDWT (all the temporal redundancy still remains in the
video). The decomposition structure generated by MDWT is shown in the
Fig~\ref{fig:forward_MDWT}. 1-iteration MDWT inputs a sequence of
images $\{F_i\}$ and outputs a sequence of decompositions $\{F_i.L\}$
and $\{F_i.H\}$ (also called sequence subbands). $L$ stands for low-frequency and $H$ for high-frequency. These subbands are the result of decompose each image $F_i$ into  producing the SRLs
$\{F_i.L\}$ (or simply $F.L$) and $\{F_i.L_0\}=\{F_i\}$. For the sake
of simplicity, we will denote sequence spatial subbands
$\{F_i.L_1\}=L_1$ and $\{F_i.H_1\}=H_1$, and the sequence spatial
resolution levels as $L_1$ and $L_0$.

\begin{figure}
  \centering \myfig{graphics/forward_MDWT}{6cm}{600}
  \lstinputlisting[firstline=15, lastline=41, language=python,
    caption=MDWT
    (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/MDWT.py}{MDWT.py}).]{../src/MDWT.py}.
  \lstinputlisting[firstline=43, lastline=65, language=python,
    caption=iMDWT
    (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/MDWT.py}{MDWT.py}).]{../src/MDWT.py}.
  \caption{Implementation of the MDWT.} %
  \label{fig:forward_MDWT}
\end{figure}

%}}}

\section{Scalability provided by MDWT}
%{{{

MDWT sequences (of (subband) decompositions) are scalable in space and
in time. Spatial scalability is a direct consequence of the 2D-DWT
(the number of SRLs can be controlled by the number of iterations of
2D-DWT). On the other hand, it is trivial to observe that MDWT
provides \emph{fully} temporal scalability (we can access to the
images randomly) because each image of the input sequence is
transformed independently.

%}}}

%{{{ Examples

\subsection*{Example: 1-iteration MDWT (\texttt{MDWT(iters=1)})}
%{{{

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Create the output directory for the 1st-level decompositions.
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/ -N 5

# Visualize the subbands.
for i in /tmp/LL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LL*.norm
for i in /tmp/LH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LH*.norm
for i in /tmp/HL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HL*.norm
for i in /tmp/HH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HH*.norm

# 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection*{Example: 2-iterations MDWT (\texttt{MDWT(iters=2)})}
%{{{

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Create the output directory for the 1st-level decompositions.
yes | cp ../sequences/stockholm/*.png /tmp

# 2-iterations MDWT.
python3 -O MDWT.py -p /tmp/ -N 5
python3 -O MDWT.py -p /tmp/LL -N 5

# 2-iterations iMDWT.
python3 -O MDWT.py -b -p /tmp/LL -N 5
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection*{Example: Interpolating with the 2D-DWT}
%{{{

If we apply an inverse transform to the original images, supposing
that they are the low-frequency subbands of the input decomposition,
and that the high-frequency subbands are zero, we can interpolate each
image of the original sequence by a factor of $2$ in each direction.

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Copy the current images as LL subbands.
rm /tmp/*.png
for i in {0..4}; do
  image_number=$(printf "%03d" $i)
  cp ../sequences/stockholm/$image_number.png /tmp/LL$image_number.png
done

# Apply the inverse DWT.
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Notice that the dynamic range of the sequence could be changed (the
# images could appear brighter of darker). This is consequence of the
# relative gain of the LL subband in respect of the rest of subbands.
rm /tmp/*.norm
for i in /tmp/???.png; do convert -normalize $i $i.norm; done
animate /tmp/*.png.norm
\end{verbatim}

%}}}

%}}}

\section{Video transform alternatives}
%{{{

To remove both, spatial and temporal redundancies, two different
alternatives are available:
\begin{enumerate}
\item In a \textbf{``t+2D'' transform}, the video is first
  \href{https://en.wikipedia.org/wiki/Digital_filter\#Analysis_techniques}{analyzed}
  (transformed) over the time domain and next, over the space domain.
\item A \textbf{``2D+t'' transform} does just the opposite.
\end{enumerate}
Each choice has a number of \emph{pros} and \emph{cons}. For example,
in a ``t+2D'' transform we can apply directly any frame\footnote{In video coding, images are commonly refered as frames.} predictor
based on \href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} because the input is a normal video. However, if we
implement a ``2D+t'' transform, the input to the motion estimator is a
sequence decompositions.
\href{http://www.polyvalens.com/blog/wavelets/theory}{The overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means that, exactly the same object
placed in two different frames in different positions will generate
different wavelet coefficients, even if the displacement is an integer
number of pixels and therefore, the pixels of the object in both
frames are identical.  Therefore, motion estimators which compare
coefficient values will not work with accuracy on the decomposition
domain. On the other hand, if we want to provide true spatial
scalability (processing only those
\href{https://www.tutorialspoint.com/dip/spatial_resolution.htm}{spatial
  resolutions} (scales) necessary to get a spatially scaled of our
video), a ``t+2D'' transformed video presents some drawbacks:

\begin{enumerate}
\item The perfect reconstruction of the original frames only is
  possible if the ``t'' stage is identical at both, the
  \href{https://en.wikipedia.org/wiki/Encoder}{encoder} and the
  \href{https://en.wikipedia.org/wiki/Decoder}{decoder}. Therefore, if
  the encoder applies ``t'' at the full resolution (scale 0), the
  decoder must also apply ``t'' at full resolution, even if the
  resolution of the reconstructed frames were smaller. This could be
  unfeasible for receivers with low computational resourcers.
\item Perfect reconstruction can be sacrified by using a quantized
  version of the motion information, but the rate/distortion (R/D)
  tradeoff will be worst. In this case, also, the decoder would be
  wasting motion information.
\item This last problem (the waste of motion information) could be
  avoided if the motion data were encoded in a progressive
  representation (suitable for working in a different spatial
  resolution of the frames). Unfortunately, progressive
  representations usually need more data than non-progressive ones.
\end{enumerate}

Finally, it is important to realize that the presence of the motion
data in the code-stream introduces also complexity into the decoding
process because, in a quality scalable scenario, for example, it is
not trivial (specially when non-linear systems such as those based on
\mylink{video_compression}{ME} are involved) to decide how to
interleave the motion and the texture information in a code-stream
that can be decoded partially, depending on the avaliable bandwidth.

%}}}

\section{DWT structures and Laplacian Pyramids}
%{{{

The downsamling performed in the DWT generates that
\href{http://www.polyvalens.com/blog/wavelets/theory}{the overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means that, two images that have exactly
the same content (pixel-by-pixel) but moved (even an integer number of
pixel positions), will produce different DWT coefficients. This, as it
will be seen after, is a problem when we want to perform comparisons
in the DWT domain.

The shift invariance is obtained if the downsampling operation is
removed, as happens in the
\href{https://ieeexplore.ieee.org/document/1408191}{Overcomplete DWT}
and in the
\href{http://fourier.eng.hmc.edu/e161/lectures/canny/node3.html}{Laplacian
  Pyramids} (LP). Fortunately, there is an equivalence between DWT
subbands and a family of Laplacian Pyramids that we will call
Orthogonal LPs (OLP), and it is quite straightforward to
\href{https://vicente-gonzalez-ruiz.github.io/pyramids-and-wavelets/}{pass
  from a decomposition representation to a pyramid representation and
  viceversa}, when the $L$ and $H$ filters are used for both, the DWT
and the LP. If the filters used for generating the LP are not
orthogonal, then it is not possible to travel from the LP dmoain to
the the DWT domain without lossing information.

%}}}

\section{Motion Compensated Orthogonal Laplacian Pyramid (MCOLP)}
%{{{

MCOLP is a ``2D+t'' transform. The ``2D'' stage is 1-iteration MDWT,
and it is applied first. After this, the ``t'' stage goes, where a
$T$-iterations 1D
\href{https://en.wikipedia.org/wiki/Motion_compensation}{Motion
  Compensated (MC)} DWT is applied, removing the temporal redundancy
between adjacent $H$ subbands. The number of iterations $T$ determines
the \href{https://en.wikipedia.org/wiki/Group_of_pictures}{GOP} size
\begin{equation}
  G=2^T.
  \label{eq:GOP_size}
\end{equation}

We can think of MCLOPT such as a MCTF applied to a given spatial
resolution level, and because the motion is estimated using the
previous lower resolution level (which is available at the decoder),
it is not necessary to send the motion information in the
code-stream. Notice also that this process can be used for any
number of spatial resolution levels, simply by using MCLOPT
iteratively on the low spatial resolution level (resulting of the
previous iteration of PRMC that applies first MCDWT).

A peculiar feature of MCLOP is that the predict step used in the
1D-DWT is performed in the Orthogonal Laplacian Pyramid domain, which
is shift invariant because the coefficients have not been subsampled
(as happens in the DWT domain).

%}}}

\subsection{The MCOLP butterfly}
%{{{

The butterly inputs three decompositions $a=\{a.L, a.H\}$, $b=\{b.L,
b.H\}$ and $c=\{c.L, c.H\}$, and outputs a residue subband
$\tilde{b}.H$, which replaces to $b.H$ in the original $b$
decomposition. Therefore, after the use of the bufferfly, we get $a$ ,
$\tilde{b}=\{b.L, \tilde{b}.H\}$ (notice that only the high-frequency
information of $b$ has been compensated) and $c$. This replacement is
fully reversible because the forward transform uses only the
information that the inverse transform will have access to. Notice
that the
\href{http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf}{pyramid
  domain} (which is invariant to the pixels displacements) has been
used to estimate and compensate the $H$ subbands.

\begin{figure}
  \centering \myfig{graphics/forward_butterfly}{10cm}{1000}
  \lstinputlisting[firstline=20, lastline=53, language=python,
    caption={MCOLP butterfly
      (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/MCOLP.py}{MCOLP.py}).}]{../src/MCOLP.py}
  \lstinputlisting[firstline=55, lastline=81, language=python,
    caption={Inverse MCOLP butterfly
      (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/MCOLP.py}{MCOLP.py}).}]{../src/MCOLP.py}
  \lstinputlisting[firstline=4, lastline=9, language=python,
    caption={Computation of the prediction
      frame (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/simple_average.py}{simple\_average.py}).}]{../src/simple_average.py}
  \caption{Forward and inverse MCOLP butterflies.}
  \label{fig:forward_butterfly}
\end{figure}

Notice that the ME projections for $[b.L]$ are performed with $[a.L]$
and $[c.L]$ instead of $a$ and $c$ (that are available at the decoder)
because, in $a$ and $c$ there is high-frequency information that can
dificult the computation of the projections $[a.L]\rightarrow [b.L]$
and $[c.L]\rightarrow [b.L]$.

%\subsection{The MCOLP step}

%\begin{figure}
%  \centering
%  \svg{forward_MCOLP_step}{1200}
%  \caption{MCOLP forward step.}
%  \label{fig:forward_MCOLP_step}
%\end{figure}
%\subsection{Forward and backward (inverse) transform}

%}}}

\subsection{The MCOLP transform}
%{{{

The $T$-iterations MCOLP is the result of applying the MCOLP butterfly
to all the frames of $T+1$ different temporal scales (see
Fig.~\ref{fig:MCOLP}). These scales are defined by the frames with
number multiple of $2^t$, where $1\leq t\leq T$ is the current
iteration.

\begin{figure}
  \centering
  %\myfig{graphics/temporal_decorrelation_2x1}{10cm}{1000}
  \lstinputlisting[firstline=83, lastline=141, language=python, caption={MCOLP (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/MCOLP.py}{MCOLP.py}).}]{../src/MCOLP.py}
  \lstinputlisting[firstline=143, lastline=184, language=python, caption={iMCOLP (\href{https://github.com/Sistemas-Multimedia/PRMC/blob/master/src/MCOLP.py}{MCOLP.py}).}]{../src/MCOLP.py}
  \caption{Implementation of the MCOLP.}
  \label{fig:MCOLP}
\end{figure}

Notice that, if $N=G+1$ (where $G$ is the GOP size, see
Eq.~\ref{eq:GOP_size}), except for the first and the last frame of the
sequence, all the $H$-subbands are temporally decorrelated.

%}}}

\subsection*{Example: 1-iteration $\times$ (MDWT + 1-iteration MCOLP) (\texttt{PRMC(iters=1)})}
%{{{

In this example, a sequence of five frames is transformed first with
the (1-iteration) MDWT (\texttt{MDWT(iters=1)}), producing 2 SRLs by
frame. Next, the $H$ subbands of the odd frames (frames 1 and 3) are
motion compensated using the (1-iteration) MCOLP
(\texttt{MCOLP(iters=1)}).

\begin{verbatim}
predictor=1
iterations=1

# You must be in the 'src' directory to run this.
rm -f /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Show the length of the subbands.
for i in /tmp/LL???.png; do ls -l $i; done
for i in /tmp/LH???.png; do ls -l $i; done
for i in /tmp/HL???.png; do ls -l $i; done
for i in /tmp/HH???.png; do ls -l $i; done

# 1-iteration MCOLP.
python3 -O MCOLP.py -P $predictor -p /tmp/ -T $iterations

# Show the length of the subbands.
for i in /tmp/LL???.png; do ls -l $i; done
for i in /tmp/LH???.png; do ls -l $i; done
for i in /tmp/HL???.png; do ls -l $i; done
for i in /tmp/HH???.png; do ls -l $i; done

# Has changed in length any of them? Remember that a change in lenght
# implies a change in content.

# Lets recover the original sequence ...
rm /tmp/???.png

# 1-iteration iMCOLP.
python3 -O MCOLP.py -P $predictor -b -p /tmp/ -T $iterations

# 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

\begin{figure}
  \centering
  \myfig{graphics/temporal_decorrelation_1x1}{10cm}{1000}
  \caption{Subbands generated after running $\mathtt{PRMC(iters=1)}$: $L$.}
  \label{fig:1xPRMC1}
\end{figure}

%}}}

\subsection*{Example: 1-iteration $\times$ (MDWT + 2-iterations MCOLP) (\texttt{PRMC(iters=2)})}
%{{{

This example is similar to the previous, but now the number of
iterations of MCOLP is $T=2$, and therefore, the second iteration of
MCOLP compensates also the $H$ subband of the frame 2. This example
matches exactly with the described in the Fig~\ref{fig:MCOLP}. Notice
that the GOP size is $4$ ($3$ TRLs) and the number of SRLs is $2$.

\begin{verbatim}
predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/*.png /tmp/tmp

# 1-iteration MCOLP.
python3 -O MCOLP.py -p /tmp/ -T 1

# Is the content of the MCOLP subbands different to the MDWT's output?
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LL$ii.png /tmp/tmp/LL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LH$ii.png /tmp/tmp/LH$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HL$ii.png /tmp/tmp/HL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HH$ii.png /tmp/tmp/HH$ii.png;echo; done

rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/*.png /tmp/tmp

# 2-iterations MCOLP.
python3 -O MCOLP.py -p /tmp/ -T 2

# Is the content of the MCOLP subbands different to the MDWT's output?
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LL$ii.png /tmp/tmp/LL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LH$ii.png /tmp/tmp/LH$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HL$ii.png /tmp/tmp/HL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HH$ii.png /tmp/tmp/HH$ii.png;echo; done

# Comparing LH visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/LH$ii.png /tmp/tmp/MDWT_LH$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/LH$ii.png /tmp/MCOLP_LH$ii.png; done
animate /tmp/tmp/MDWT_LH00?.png &
animate /tmp/MCOLP_LH00?.png &

# Comparing LH002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCOLP > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/LH002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/LH002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Comparing HL visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/HL$ii.png /tmp/tmp/MDWT_HL$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/HL$ii.png /tmp/MCOLP_HL$ii.png; done
animate /tmp/tmp/MDWT_HL00?.png &
animate /tmp/MCOLP_HL00?.png &

# Comparing HL002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCOLP > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/HL002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/HL002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Comparing HH visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/HH$ii.png /tmp/tmp/MDWT_HH$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/HH$ii.png /tmp/MCOLP_HH$ii.png; done
animate /tmp/tmp/MDWT_HH00?.png &
animate /tmp/MCOLP_HH00?.png &

# Comparing HH002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCOLP > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/HH002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/HH002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Now we recover the original video.

# 2-iterations iMCOLP.
python3 -O MCOLP.py -P $predictor -b -p /tmp/ -T $iterations

# 1-iteration iMDWT
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png

python3 ../tools/show_statistics.py -i /tmp/diff_002.png
\end{verbatim}

\begin{figure}
  \centering
  \myfig{graphics/temporal_decorrelation_1x2}{10cm}{1000}
  \caption{Subbands generated after running $1\times\mathtt{PRMC(iters=2)}$.}
  \label{fig:1xPRMC2}
\end{figure}

%}}}

\end{document}

\subsection*{Example: 1-iteration $\times$ (MDWT + 4-iterations MCOLP) (\texttt{PRMC(iters=1, olp\_iters=4)})}
%{{{

An example with $G=2^3=16$, i.e. $5$ TRLs. $2$ SRLs.

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png
video_file=/tmp/akiyo_cif.y4m
if test -f $video_file; then
  print akiyo exists
else
  wget https://media.xiph.org/video/derf/y4m/akiyo_cif.y4m -O /tmp/akiyo_cif.y4m
fi
mplayer /tmp/akiyo_cif.y4m
ffmpeg -i /tmp/akiyo_cif.y4m -vframes 17 -start_number 0 /tmp/%03d.png

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/ -N 17

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/LL*.png /tmp/tmp
cp /tmp/LH*.png /tmp/tmp
cp /tmp/HL*.png /tmp/tmp
cp /tmp/HH*.png /tmp/tmp

# 4-iterations MCOLP.
python3 -O MCOLP.py -p /tmp/ -T 4 -N 17

ls -l /tmp/LH*.png > /tmp/1
ls -l /tmp/tmp/LH*.png > /tmp/2
paste /tmp/1 /tmp/2

ls -l /tmp/HL*.png > /tmp/1
ls -l /tmp/tmp/HL*.png > /tmp/2
paste /tmp/1 /tmp/2

ls -l /tmp/HH*.png > /tmp/1
ls -l /tmp/tmp/HH*.png > /tmp/2
paste /tmp/1 /tmp/2

# Now we recover the original video.

# 4-iterations iMCOLP.
python3 -O MCOLP.py -b -p /tmp/ -T 4 -N 17

# 1-iteration iMDWT
python3 -O MDWT.py -b -p /tmp/ -N 17

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..16}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png

python3 ../tools/show_statistics.py -i /tmp/diff_002.png
\end{verbatim}

The (temporal and spatial) subbands generated after using $\mathtt{PRMC(1)$ are:
\begin{enumerate}
\item $L^0_1=\{F_0.L_1, \cdots, F_4.L_1\}$ which when decoded, reconstruct the sequence of frames with a resolution $Y/2\times X/2$ at the original frame rate.
\item $H^$L^1_0=\{F_0.L_0, F_2.L_0, F_4.L_0\}$ which when decoded, reconstruct frames $F_0$, $F_2$, and $F_4$ with a resolution $Y\times X$.
\item $L^0_0=\{F_0.L_0, \cdots, F_4.L_0\}$ 
\end{enumerate}

%\begin{verbatim}
%L^
%\end{verbatim}
%  \caption{Subbands generated after running $1\times\mathtt{PRMC(iters=2)}$.}
%  \label{fig:1xPRMC2}
%\end{figure}

%}}}

\subsection{Scalability provided by MCOLP}
%{{{

MCOLP preserves the dyadic spatial decomposition generated by MDWT,
and therefore, MCOLP provides the same spatial scalability than MDWT.

Unfortunately, as the rest of video encoders based on MC, MCOLP
reduces the temporal scalability provided by MDWT, allowing (in the
case of MCOLP) only a dyadic access to the frames. For example, after
applying each iteration of the inverse MCOLP, if $T=3$ (the GOP size
$G=8$, i.e. $4$ temporal resolution levels (TRLs) have been
generated), the frames of the second GOP (GOP 1, considering that the
first GOP (GOP 0) is only composed by the first frame of the sequence)
should be reconstructed in this order:

\begin{verbatim}

GOP0          GOP 1
---- ------------------------
  s0                       s8  <- output of the 3-iterations MCOLP. Provides TRL3.
               s4              <- first iteration of the inverse MCOLP. Provides TRL2.
         s2          s6        <- second iteration. Provides TRL1.
      s1    s3    s5    s7     <- third iteration. Provides TRL0.

\end{verbatim}

%}}}

\subsection*{Example: 3-iters $\times$ (MDWT + 2-iters MCOLP) (\texttt{PRMC(iters=3, olp\_iters=2)})}
%{{{

\begin{verbatim}
predictor=1
iterations=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py  -p /tmp/
python3 -O MCOLP.py -p /tmp/     -P $predictor -T $iterations
python3 -O MDWT.py  -p /tmp/LL
python3 -O MCOLP.py -p /tmp/LL   -P $predictor -T $iterations
python3 -O MDWT.py  -p /tmp/LLLL
python3 -O MCOLP.py -p /tmp/LLLL -P $predictor -T $iterations

rm /tmp/00?.png

python3 -O MCOLP.py -p /tmp/LLLL -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/LLLL               -b
python3 -O MCOLP.py -p /tmp/LL   -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/LL                 -b
python3 -O MCOLP.py -p /tmp/     -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/                   -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

The GOP size is $4$, generating $3$ TRLs in this case. $4$ is the number of
SRLs.

\begin{verbatim}

TRLs:

GOP0      GOP 1
---- -------------
  s0           s4  <- TRL1 U {s0, s4} = TRL2
         s2        <- TRL0 U s2 = TRL1
      s1    s3     <- TRL0

SRLs:

+---+---+-------+---------------+
| 3 | 2 |   1   |       0       |
+---+   |       |               |
|       |       |               |
+-------+       |               |
|               |               |
|               |               |
|               |               |
+---------------+               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
+-------------------------------+
\end{verbatim}

$3\times 4=12$ different possible reconstructions of the video are possible:
\begin{enumerate}
\item s0.3, s4.3.
\item s0.2, s4.2.
\item s0.1, s4.1.
\item s0.0, s4.0.
\item s0.3, s4.3, s2.3.
\item s0.2, s4.2, s2.2.
\item s0.1, s4.1, s2.1.
\item s0.0, s4.0, s2.0.
\item s0.3, s4.3, s2.3, s1.3, s3.3.
\item s0.2, s4.2, s2.2, s1.2, s3.2.
\item s0.1, s4.1, s2.1, s1.1, s3.1.
\item s0.0, s4.0, s2.0, s1.0, s3.0.
\end{enumerate}

%}}}

%{{{ Temporal interpolation

\begin{comment}
  \subsection{Temporal interpolation}

Hay que trasladar el lifting a este nuevo dise√±o. As√≠ se sabr√° mejor c√≥mo realizar la interpolaci√≥n temporal.
  
  Si conseguimos a->c y c->a, siendo a y c dos im√°genes adyacentes (000 y 001, por ejemplo), podemos generar la imagen predicha proyectando a usando (a->c)/2 y c usando (c->a)/2. As√≠ conseguimos 2 proyecciones, con las que podemos hacer la media o usar el error de predicci√≥n para calcular la proporci√≥n de cada proyecci√≥n.
\begin{verbatim}
predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCOLP.py -p /tmp/     -P $predictor -T 1

rm /tmp/00?.png

mv /tmp/LL004.png /tmp/LL008.png
mv /tmp/LL003.png /tmp/LL006.png
mv /tmp/LL002.png /tmp/LL004.png
mv /tmp/LL001.png /tmp/LL002.png
mv /tmp/LH004.png /tmp/LH008.png
mv /tmp/LH003.png /tmp/LH006.png
mv /tmp/LH002.png /tmp/LH004.png
mv /tmp/LH001.png /tmp/LH002.png
mv /tmp/HL004.png /tmp/HL008.png
mv /tmp/HL003.png /tmp/HL006.png
mv /tmp/HL002.png /tmp/HL004.png
mv /tmp/HL001.png /tmp/HL002.png
mv /tmp/HH004.png /tmp/HH008.png
mv /tmp/HH003.png /tmp/HH006.png
mv /tmp/HH002.png /tmp/HH004.png
mv /tmp/HH001.png /tmp/HH002.png

python3 -O MCOLP.py -b -p /tmp/ -N 9 -P $predictor -T 2
python3 -O MDWT.py  -b -p /tmp/ -N 9

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png



predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCOLP.py -p /tmp/     -P $predictor -T 1

rm /tmp/00?.png

python3 -O MCOLP.py -p /tmp/     -P $predictor -b -T 2
python3 -O MDWT.py  -p /tmp/                   -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png



rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp/LL000.png
cp ../sequences/stockholm/000.png /tmp/LL001.png
cp ../sequences/stockholm/001.png /tmp/LL002.png
cp ../sequences/stockholm/001.png /tmp/LL003.png
cp ../sequences/stockholm/002.png /tmp/LL004.png
cp ../sequences/stockholm/002.png /tmp/LL005.png
cp ../sequences/stockholm/003.png /tmp/LL006.png
cp ../sequences/stockholm/003.png /tmp/LL007.png
cp ../sequences/stockholm/004.png /tmp/LL008.png
cp ../sequences/stockholm/004.png /tmp/LL009.png
python3 -O MCOLP.py -p /tmp/ -N 9 -b
\end{verbatim}        
\end{comment}

%}}}

%{{{ Subpixel accuracy

\begin{comment}
\subsection{Accuracy of the motion compensation (TO BE IMPLEMENTED)}
The movement of the objects in the scene captured by a video is a
continuous event. Conversely, the digital frames taken from such
objects are discrete in the time and the space domains. This means
that, in general, two different positions of an object in two
different frames is not going to match with a distance that can be
measured by an integer number of pixels. For example, lets suppose that the content of the frame $s_0$ is:
\begin{verbatim}
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
\end{verbatim}
and that the camera has moved, following a parallel movement to the
surface of the object of 1/2 pixels. In this case, the frame $s_1$
taken from such object would be:
\begin{verbatim}
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
\end{verbatim}
and it can be seen, it is difficult to determine that the object has
been moved, because the values of the pixels have changed
significantly. However, if we interpolate both frames using the
wavelet ``bior3.5'' of PyWavelets, we obtain for the first
frame\footnote{\texttt{pywt.idwt([0,0,0,255,0,0,0],None,'bior3.5',
    'per')}}:
\begin{verbatim}
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
\end{verbatim}
and for the second
one\footnote{\texttt{pywt.idwt([0,0,128,128,0,0,0],None,'bior3.5',
    'per')}}:
\begin{verbatim}
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
\end{verbatim}
that is more suitable for determining the sub-pixel accuracy motion.

So, to increase the accuracy of the MC step to 1/2 sub-pixel accuracy
(for example), we can interpolate\footnote{Considering the unkown
  high-frequency information as zero.} all the subbands that are
involved in the MC process by means of a 2-iterations iDWT, generating
$[a.L]^2$, $[c.L]^2$, $[a.H]^2$, $[b.H]^2$ and $[c.H]^2$, and finally,
using a 2-iterations DWT, to obtain $\tilde{b}$.
\end{comment}

%}}}

%\subsection{(Spatial) Multiresolution}
%Spatial dyadic multiresolution can be obtained by appliying a sequence
%of (MDWT+MCOLP) steps (see Fig.~\ref{fig:multiresolution}).

%\begin{figure}
%  \centering %
%  \myfig{graphics/multiresolution}{10cm}{1200}
%  \caption{MDWT+MCOLP multiresolution procedure. The input to the second
%    iteration of the MDWT+MCOLP transform is the output of a previous
%    (first) iteration MDWT+MCOLP transform. The second iteration is only
%    applied to the scale 1.} %
%  \label{fig:multiresolution}
%\end{figure}

%Spatial dyadic multiresolution can be obtained by appliying a sequence
%of MDWT+MCOLP steps (see Fig.~\ref{fig:multiresolution}).

\section{Adaptive motion compensation based on the distortion of the prediction error}
%{{{

As can be seen in the MCOLP bufferfly (see
Fig.~\ref{fig:forward_butterfly}), both predictions $[b_a.H]$ and
$[b_c.H]$ have the same weight to build the prediction
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H] + [b_c.H]}{2}.
\end{equation}
This simple computation, that can have very effective for example in
\href{https://biteable.com/blog/tips/video-transitions-effects-examples/}{dissolves}
video transitions, can also be counterproductive when objects appear
only in one of the reference frames. For this reason, in this section
a different predictor is proposed, based on the estimated prediction
error generated throughout the ME process ($[\tilde{b}.H]$ in the
Fig.~\ref{fig:forward_butterfly}). To estimate this error, we will
suppose that the prediction error generated for the high frequencies
of an frame is proportional to the prediction error generated for the
low frequencies of such frame.

Let $[b_a.L]$ the prediction generated for the subband $[b.L]$ using
$[a.L]$ as reference and $[a.L]\rightarrow [b.L]$ as motion, and let
$[b_c.L]$ the prediction generated for $[b.L]$ using $[c.L]$ as
reference and $[c.L]\rightarrow [b.L]$ as motion. We now compute the
prediction errors
\begin{equation}
  \begin{array}{l}
    {[e_a.L]} = [b.L] - [b_a.L]\\
    {[e_c.L]} = [b.L] - [b_c.L].
  \end{array}
\end{equation}

We define the similarity matrices as
\begin{equation}
  \begin{array}{l}
    {[s_a.L]} = \frac{1}{1+{|[e_a.L]|}}\\
    {[s_c.L]} = \frac{1}{1+{|[e_c.L]|}}.    
  \end{array}
  \label{eq:weighted_prediction}
\end{equation}
Notice that, if (for example) the error $[e_a.L]_{x,y}=0$, the
similarity is $[s_a.L]_{x,y}=1$ (the maximum similarity), if the error
is high, the similarity tends to be low (but never $0$).

With this information, that can be recovered by the decoder, the
improved prediction is defined as
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H][s_a.L]+[s_c.L][b_c.H]}{[s_a.L]+[s_c.L]}.
\end{equation}
The Fig.~\ref{fig:weighted_average} shows an implementation of this
equation.

Notice that, if (for example) $[s_a.L]_{x,y}=1$ and
$[s_c.L]_{x,y}\approx 0$, then
$[\hat{b.H}]_{x,y} \approx [b_a.H]_{x,y}$, and viceversa. If
$[s_a.L]_{x,y}=[s_c.L]_{x,y}$, then
$[\hat{b.H}]_{x,y}=\frac{[b_a.H]_{x,y}+[b_c.H]_{x,y}}{2}$ (even if both similarities are small).

\begin{figure}
  \centering \lstinputlisting[firstline=4, lastline=15,
    language=python, caption={\href{https://github.com/Sistemas-Multimedia/MCOLP/blob/master/src/weighted_average.py}{weighted\_average.py}.}]{../src/weighted_average.py}
  \caption{Computation of the weighted average prediction.}
  \label{fig:weighted_average}
\end{figure}

%}}}

\section{Lossy compression: quantization}
%{{{

\mylink{quantization}{Quantization} allows to remove information from
signals. We will use it in MCOLP to achieve higher compression ratios,
but at the expense of losing visual information (ideally, the less
relevant information should be removed first).

When the signal cannot be recoverd perfectly, we can use a
\mylink{distortion_metrics}{distortion metric} to find how much error
has been generated.

%}}}

\subsection*{Example: Effects caused by the total attenuation of the motion compensated (MCed) $H$ subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp


# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# 1D 1-iteration MCOLP.
python3 -O MCOLP.py -p /tmp/

# Let's delete the motion compensated subbands and reconstruct the sequence ...
rm -f /tmp/LH001.png
rm -f /tmp/HL001.png
rm -f /tmp/HH001.png
rm -f /tmp/LH002.png
rm -f /tmp/HL002.png
rm -f /tmp/HH002.png
rm -f /tmp/LH003.png
rm -f /tmp/HL003.png
rm -f /tmp/HH003.png

# 1D 1-iteration iMCOLP.
python3 -O MCOLP.py -P $predictor -b -p /tmp/

# 1D 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

# Visualization of the differences with the original sequence.
rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

# MSE per frame
for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done

# PSNR per frame
for i in {0..4}; do ii=$(printf "%03d" $i); PSNR=`python3 -O ../tools/PSNR.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "PSNR(%s)=%s\n" $ii $PSNR; done
\end{verbatim}

%}}}

\subsection*{Example: Effects caused by the quantization of MCed $H$ subbands}
%{{{

\begin{verbatim}
q_step=32
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done
\end{verbatim}

%}}}

\subsection*{Example: Quantization of all $H$ subbands (MCed and intra)}
%{{{

\begin{verbatim}
q_step=32
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH004.png -o /tmp/LH004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL004.png -o /tmp/HL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH004.png -o /tmp/HH004.png -q $q_step

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done
\end{verbatim}

%}}}

\subsection*{Example: Quantization of all subbands}
%{{{

\begin{verbatim}
q_step=128
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

python3 ../tools/quantize.py -i /tmp/LL000.png -o /tmp/LL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL001.png -o /tmp/LL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL002.png -o /tmp/LL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL003.png -o /tmp/LL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL004.png -o /tmp/LL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH004.png -o /tmp/LH004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL004.png -o /tmp/HL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH004.png -o /tmp/HH004.png -q $q_step

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b 

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done
\end{verbatim}

%}}}

\section{Bit allocation}
En la mariposa (o en un esquema IPPP...) la subbanda $\tilde{H}_b$ compensada depende de las subbandas $L_a^{[q]}$, $L_b$ y $L_c$, para generar los campos de movimiento adecuados. $\tilde{H}_b$ tambi√©n depende de $H_a$ y $H_b$ 

\subsection*{Example: Lossy compression of the MC $H$ subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

quality=50
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection*{Example: Lossy compression of all $H$ subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

quality=10
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640

python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 ../tools/compress.py -i /tmp/LH004.png -o /tmp/LH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH004.png -o /tmp/LH004.png -f 32640
python3 ../tools/compress.py -i /tmp/HL004.png -o /tmp/HL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL004.png -o /tmp/HL004.png -f 32640
python3 ../tools/compress.py -i /tmp/HH004.png -o /tmp/HH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH004.png -o /tmp/HH004.png -f 32640

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection*{Example: Lossy compression of all subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

quality=10
python3 ../tools/compress.py -i /tmp/LL000.png -o /tmp/LL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL000.png -o /tmp/LL000.png -f 32640
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640

python3 ../tools/compress.py -i /tmp/LL001.png -o /tmp/LL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL001.png -o /tmp/LL001.png -f 32640
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LL002.png -o /tmp/LL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL002.png -o /tmp/LL002.png -f 32640
python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LL003.png -o /tmp/LL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL003.png -o /tmp/LL003.png -f 32640
python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 ../tools/compress.py -i /tmp/LL004.png -o /tmp/LL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL004.png -o /tmp/LL004.png -f 32640
python3 ../tools/compress.py -i /tmp/LH004.png -o /tmp/LH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH004.png -o /tmp/LH004.png -f 32640
python3 ../tools/compress.py -i /tmp/HL004.png -o /tmp/HL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL004.png -o /tmp/HL004.png -f 32640
python3 ../tools/compress.py -i /tmp/HH004.png -o /tmp/HH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH004.png -o /tmp/HH004.png -f 32640

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\section{MCOLP decomposition}
%{{{

\begin{figure}
  \centering %
  \myfig{graphics/MCOLP_decomposition}{12cm}{1200} %
  \caption{MCOLP($S=1$, $T=3$) decomposition (notice that
    $\text{GOP\_size}=8=2^T$). The arrows indicate frame dependencies
    (for example, to decode frame 1, frames 0 and 2 should have been
    decoded.} %
  \label{fig:MCOLP_decomposition}
\end{figure}

MCOLP($S$, $T$), where $S$ is the number of levels of the spatial
transform and $T$ is the number of levels of the temporal transform,
decomposes a sequence of frames into a sequence of subbands organized
in $T+1$ of temporal scales of $S+1$ spatial scales (see
Fig.~\ref{fig:MCOLP_decomposition}).

%}}}

\section{Encoding of B-type H-subbands (experimental)}
%{{{

The subband $\tilde{b}.H$ generated by the MCOLP butterfly (see
Sec.~\ref{sec:butterfly}) is no longer needed for applying the
butterfly to different decompositions and scales. For this reason, we
can compress this subband. As a result, after using the butterfly to
<<<<<<< all the decompositions of the sequence and all the scales, all the $H$
subbands can be compressed, excepting the intra-coded frame of each
GOP. Notice that the redundancy exploited in B-type $H$ subbands is
temporal.

A $\tilde{b}.H$ subband can be compressed by sorting its coefficients
by energy and predicting them. Thus, the better the prediction the
higher the compression ratio. To sort the coefficients of subband
$\tilde{b}.H$ by energy without using it (remember that the decoder
does not know this information), we can think that there is a
correlation between the coefficients of $\tilde{b}.H$ and
$\tilde{b}.L$, that is, the prediction error resulting of substracting
to $b.L$ a prediction $\hat{b}.L$ generated with the same motion
information used to built the prediction $\hat{b}.H$. Such idea has
been implemented in the following algorithm:

\begin{enumerate}
\item [1.] Compute the prediction error for the $[b.L]$ subband
\begin{equation}
  [\tilde{b}.L] = [b.L] - [\hat{b}.L]
\end{equation}
where (see Eq.~\ref{eq:weighted_prediction})
\begin{equation}
  [\hat{b}.L] = \frac{[b_a.L][s_a.L]+[s_c.L][b_c.L]}{[s_a.L]+[s_c.L]}.
\end{equation}

\item [2.] Compute the 2D-DWT of subband $L$
  \begin{equation}
    \tilde{b}=\text{DWT}([\tilde{b}.L]).
  \end{equation}

\item [3.] Find the indices for $\tilde{b}.L$ that sorts it in descending order by energy
  \begin{equation}
    \text{indices}=\text{unravel\_index}(\text{argsort}(\text{abs}(\tilde{b}.L),
    b.L.\text{width}, b.L.\text{height})).
  \end{equation}
%\item [3.] For each coefficient $x,y$ in $\tilde{b}.L$:
%  \begin{enumerate}
%  \item [a.] $d_{i=x\times b.\text{width}+y} = (\text{abs}(\tilde{b}_{x,y}), x, y)$
%  \end{enumerate}
%\item [4.] Sort $d$ in desceding order sorted by field 0 /* use the amplitude of luminance */.
\item Go over $\tilde{b}.L$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy.
%\item [5.] Estimate the nearest power of two smaller or equan than the maximum amplitude
%  \begin{equation}
%    \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(d_{0}))).
%  \end{equation}
%\item [6.] For each coefficient $i$ in $d$:
%  \begin{enumerate}
%  \item [a.] $x,y = d_i[1:2]$.
%  \item [b.] If any of
%    $1<<\text{int}(\log_2(\tilde{b.H}_{x,y}))==\lambda_0$ then send
%    $\text{sign}(\tilde{b.H}_{x,y})$ for the three (LH, HL and HH)
%    subbands.
%  \item [b.] Send $(\hat{b.H}_{x,y}~\text{bitwise-and}~\lambda)/\lambda$ for
%    the three (LH, HL and HH) subbands.
%  \end{enumerate}
\item [7.] $\lambda >>= 1$.
\item [8.] Goto step 6.
\end{enumerate}

%}}}

\section{Encoding of I-type H-subbands (experimental)}
%{{{

After a $T$-levels MCOLP transform of a sequence, $T+1$ temporal
subbands are generated, and the temporal subband $L^{T+1}$ has a one
I-type subband for each GOP (see
Fig~\ref{fig:MCOLP_decomposition}). Notice that, in a I-type frame,
the redundancy exploited to accumulate the visual information in the
$L$ subband (or to remove the visual information in the $H$ subbands)
is spatial.

After using MCOLP, I-type frames are decomposed into
the four subbands $LL$ (low-frequencies), $LH$, $HL$ and $HH$
(high-frequencies). Depending on the frame content and wavelets used
in the 2D-DWT, $H$ subbands contain a certain amount of \emph{visual
  information}\footnote{Any visual stimulus that provides information
  to humans}. The key here is to reduce as much as possible such
visual information, leaving in the $H$ subbands only \emph{visual
  noise}\footnote{Any visual stimulus that does not provides
  information to humans.} If we achieve this, the $H$ subbands,
expressed in a sign/magnitude representation will have two features:
(1) the probability of finding zeros when we are moving from the least
significant bit-planes fo the most significant ones of the magnitude
of the coefficients will increase (to the point where from one
bit-plane upwards, all the bits will be zero), and (2) the correlation
in the sign bit-plane will be zero. In this situation, an efficient
encoding method is to compress the bit-planes, starting at the MSBP
with at least one bit to 1 (in other words, ``send'' the ones of the
MSBP), sending before the corresponding sign. If more than one one is
found in the MSBP, the bits should be processed by descending
magnitude, i.e., sending first those bits that corresponds with
coefficients with a higher magnitude. This information can be
extracted from the $L$ subband as below is shown.

Information in the $L$ subband can be used to futher reduce the visual
information of $H$ subbands and to estimate the value of its
high-frequency coefficients, and therefore to sort them (at least
approximately) by their magnitude. The idea is to predict the $H$
coefficients by introducing some (visual) information in the $[L]$
subband, and to perform again the 2D-DWT (using the same
wavelets). This generates a $\hat{H}$ subbands that substracted to $H$
should remove the visual information from them (decorrelating the
signs and reducing the number of bits necessary to represent most of
the coefficients).
\begin{equation}
  \tilde{H} = H - \hat{H}
\end{equation}

$\tilde{H}$ is as a prediction error that basically should store
noise. Therefore, if $\tilde{H}$ is not transmitted at all, a good
approximation of $L^0$ should be recontructed. However, in most of
cases, some amount of (unpredictable) visual information will remain
in $\tilde{H}$, concentrated in the MSBPs. The current representation
of the rest of BPs (with unpredictable bits) should be
efficient. Sumarizing, $\tilde{H}$ is a triple subbands of spatially
uncorrelated coefficients, most of them small, with zeros in the MSBPs
and random bits in the LSBPs, and to progressively encode them, a BP
compressor can be used.

At this point, any BLIC (Bi-Level Image Compressor) should be
applicable. However, there is a dependencies between the BPs that
using a BLIC that cannot be exploited. The question is, could it be
possible to predict the coordinates of the ones in a BP? (by default
the BPs are zero). If this is possible, a sequence of bits (with so
many bits as coefficients are in $H$) can encode the success of such
prediction. Suppose that $\hat{H}\downarrow$ (the list of indexes the
sorts $\hat{H}$ in descending order by amplitude) determines the
possible localization of the most energetic coefficients in
$\tilde{H}$. If so, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ (the list of residue coefficients
(approximately-) sorted by energy) will be 1 (with a high probability)
and the rest will be 0 (with a high probability). For example, if
there are five 1's in the MSB of the beginning of
$\tilde{H}[\hat{H}\downarrow]$ (that is, a perfect prediction in which
we can localize the position of the most energetic coefficients, but
not its magnitude that is, if their MSB is 1 or 0), the complete BP
can be encoded with the symbols <5><0> (five 1's localized in the
first five positions of $\tilde{H}[\hat{H}\downarrow]$). If for
example, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ are 0101001000110... (obviously this
prediction is not perfect), then the output symbols are
<0><1><1><1><2><1><3><2><0>, which can be re-encoded as
<0><1,3><2,1><1,1><3,1><2,1>,<0>.

The output of the last stage is a sequence of symbols with a
exponiental probability distribution that can be encoded efficiently
with a variable-length code. Or using DPCM?

The perfect prediction of the 2nd MSBP of
$\tilde{H}[\hat{H}\downarrow]$ if formed by the first refinements bits
of those coeffientes that already are significative, and by the first
1 of those coefficients that start to being significative in this 2nd
MSBP. Such perfect prediction has the structure
1.(5).10.(7).01.(10).10...0. We can realize now that the first two
runs has a length equal to the number of coefficients significative in
the previous BP, so, we only need to encode the first run of 1's,
generating the symbols <5><0><10><0>. Let's suppose an unperfect
prediction like 1101100|0111100110101100001010...., and that the
number of significant coefficients is 7. In this case, the output is
<2><1><2><0><0><4><2><2><1><1><1><2><4><1><1><1><1><0>.

In the extreme case where the prediction is completely wrong, we are
going to generate a secuence of $N$ bits 0101010101... which can be
encoded as <0><1><1>...<1> ($N$ symbols), which can be re-encoded as
<0><1,N-1>.

%By definition, unpredictable information can not be
%compressed. Consecuently, the lossless encoding of $H$ is basically:
%(1) determine the

The coefficients of $\hat{H}$ sorted by their magnitude in descending
order can be used to send first those bits that belong to the largest
coefficients of $H$.

Obviously, the prediction
\begin{equation}
  \text{sort}(H)==\text{sort}(\hat{H})
\end{equation}
will not always be perfect. This can have two different consequences: (1) that 

Introduced such ideas, to compress the $H$ subbands of a 1-level
2D-DWT, the following algorithm can be used:

\begin{enumerate}

\item Compute $[L]$, zooming-in the $L$ subband.
  \begin{equation}
    [L] = \text{iDWT}(L, 0)
  \end{equation}
  
\item Add to $[L]$ some visual information $V$ that could be present
  in original $L^0$. Any edge enhancement algorithm should work.
  \begin{equation}
    [L'] = [L] + V
  \end{equation}

\item Compute the 2D-DWT of $[L']$ to obtain a prediction $\hat{H}$.
  \begin{equation}
    \_, \hat{H} = \text{DWT}([L'])
  \end{equation}
  Notice that $\_\approx L$.

\item Substract the prediction to the $H$ subbands.
  \begin{equation}
    \tilde{H} = H-\hat{H}
  \end{equation}

\item Sort $\hat{H}$ by magnitude in descending order. For most
  frames, $\hat{H}\approx H$ and therefore, the most significant
  coefficients in both structures should be placed in the same
  coordinates.
  \begin{equation}
    \hat{H}\downarrow = \text{unravel\_index}(\text{argsort}(\text{abs}(\hat{H})))
  \end{equation}

\item Go over $\hat{H}\downarrow$, sending by bit-planes the
  sign-magnitude representation of the coefficients of $H$.

  \begin{enumerate}

  \item Estimate the nearest power of two, smaller or equal than the
    coefficient of $H$ with the maximum amplitude.
    \begin{equation}
      \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(H_{\hat{H}\downarrow[0]}))).
    \end{equation}
    $\lambda$ should be the index of the MSBP in $H$.
    
  \item Send the BP of index $\lambda$.
    \begin{equation}
      \begin{array}{l}
        \text{for~}H_{s,x,y}\text{~in} H[\hat{H}\downarrow]: \\
        ~ if \lambda > int(\log_2(abs(H_{s,x,y}))) > \lambda/2: \\
        ~~ send(sign(H_{s,x,y}))\\
        ~ 
        %~ 
        %\text{for~}d\text{~in range}(3): \\
        %~ \text{for~}y\text{~in range}(L^0.\text{height}): \\
        %~~ \text{for~}x\text{~in range}(L^0.\text{width}): \\
        %~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
        %~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
      \end{array}
    \end{equation}
  \end{enumerate}
\end{enumerate}

%}}}
  
%{{{ To compress the $a.H$ subband

To compress the $a.H$ subband in the last iteration of the butterfly,
the following algorithm can be used:
\begin{enumerate}
\item [1.] Compute the 2D-DWT of the $a.L$ subband
\begin{equation}
  LL^2, LH^2, HL^2, HH^2 = \text{DWT}(LL). 
\end{equation}

\item [2.] Desplace 2 bits to the left the $H$ coefficients to create
  space for encoding the subband
  \begin{equation}
    \begin{array}{l}
      LH^2 <<= 2 \\
      HL^2 <<= 2 \\
      HH^2 <<= 2.
    \end{array}
  \end{equation}
\item [3.] Label the coefficients of the subbands:
  \begin{equation}
    \begin{array}{l}
      HL^2 += 1 \\
      HH^2 += 2.
    \end{array}
  \end{equation}
\item [4.] Create an array with the 3 matrices
  \begin{equation}
    H = [LH^2, HL^2, HH^2].
  \end{equation}
\item [5.] Create a linear array with the 3 flattened matrices
  \begin{equation}
    H'=\text{ravel}(H).
  \end{equation}
\item [6.] Find the indices for the $LH^2$, $HL^2$ and $HH^2$ matrices
  that sort $H'$ in descending order by energy
  ($\text{width}=LH^2.\text{shape}[0]$ and $\text{height}=LH^2.\text{shape}[1]$)
  \begin{equation}
    \text{indices} =
    \text{unravel\_index}(\text{argsort}(\text{abs}(H')), \text{width},
    \text{height}).
  \end{equation}
\item [7.] Go over $H'$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy
  \begin{equation}
    \begin{array}{l}
      \text{for~}d\text{~in range}(3): \\
      ~ \text{for~}y\text{~in range}(\text{height}): \\
      ~~ \text{for~}x\text{~in range}(\text{width}): \\
      ~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
      ~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
    \end{array}
  \end{equation}
\end{enumerate}

%}}}

\section{Progressive reconstruction}
%{{{

The forward bufferfly should reduce the energy of subband $b.H$$ after
substracting a prediction (see Eq.~\ref{eq:weighted_prediction}) which
is generated using the subbands $a.H$ and $c.H$ as references. If the
prediction is good, the energy of $\tilde{b.H}$ will be small and
viceversa. If the prediction is good, the quantization of the subbands
$a.H$, $b.H$ and $c.H$ should leave more energy in the subbands $a.H$
and $c.H$ than in $b.H$. Therefore, if the prediction is good, the
progressive reconstruction of the subbands (using a progressively
small quantization step) should reconstruct first the information of
$a.H$ and $c.H$.

%}}}

\section{Coefficients encoding}
%{{{

Subband $\tilde{b.H}$ is not used any more as a reference and
therefore, it can be compressed. The compression algorithm sort the
coefficients of $\tilde{b.H}$ by the amplitude of the coefficients of
$\tilde{b.L}$, considering that the prediction error will affect in
the same way the low and the high frequencies of $b$. Then, the sorted
coefficients are DPCM encoded, quantized, and entropy encoded.

%}}}

\section{Multilevel MCLT}
%{{{

The distance between frames $a$, $b$ and $c$ should decrease the
efficiency of the predictions, generating more energy in the
low-frequency subbands. Therefore, the quantization when $T>1$ should
transmit more information of the low-frequency subbands than of the
high-frequency ones when the subbands are quantized.

%}}}
