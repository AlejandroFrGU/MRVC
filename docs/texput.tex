% Emacs, this is -*-latex-*-

\title{Motion Compensated Discrete Wavelet Transform (MCDWT)}

\author{Vicente Gonz√°lez Ruiz}

\maketitle
\tableofcontents

\section{Intro}
%{{{

\href{https://en.wikipedia.org/wiki/Video}{Video} data contain high
amounts of redundancy, spatial and temporal. For this reason, most of
video encoders compress the input sequece of
\href{https://en.wikipedia.org/wiki/Digital_image}{images} (which are
matrices of \href{https://en.wikipedia.org/wiki/Pixel}{pixels})
basically in two stages: (1) a transform stage in the
\href{https://en.wikipedia.org/wiki/Time_domain}{time} and in the
\href{https://www.quora.com/What-is-spatial-domain-in-image-processing}{spatial}
domains that produces a collection of spatially
\href{https://en.wikipedia.org/wiki/Decorrelation}{uncorrelated}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{coefficients},
and (2) an
\href{https://en.wikipedia.org/wiki/Entropy_encoding}{entropy
  encoding} phase which removes the statistical redundancy that can
still remains after the decorrelation (transform). These coefficients
have two interesting features:
\begin{enumerate}
\item Usually, a \textbf{
  \href{https://vicente-gonzalez-ruiz.github.io/symbol_compression/}{smaller
  entropy}} than the original pixels. This helps to increase
  the \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
  ratio}.
\item The transform domain provides a
  \textbf{\href{https://en.wikipedia.org/wiki/Image_resolution}{multiresolution}
    representation (spatial and temporal)} of the visual information (spatial
  \href{http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf}{scalability}).
\end{enumerate}

%}}}

\section{The Distrete Wavelet Transform (DWT)}
%{{{

\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{(2D-)DWT}
is a digital transform that, applied to an image, performs spatial
decorrelation (2D) and obtains a
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/index.html#x1-3500024}{multiresolution
  dyadic representation} of such image, conforming a collection of DWT
subbands. The forward transform converts the image into a set of
frequency subbands with coefficients representing different spatial
areas and frequency orientations. There is an equivalence between
(DWT) subbands (a \emph{(subband) decomposition}) and
\href{http://fourier.eng.hmc.edu/e161/lectures/canny/node3.html}{Laplacian
  pyramids}, and it is quite simple to
\href{https://vicente-gonzalez-ruiz.github.io/pyramids-and-wavelets/}{pass
  from a decomposition representation to a pyramid representation and
  viceversa}.

\href{https://github.com/vicente-gonzalez-ruiz/MCDWT/blob/master/src/DWT.py}{DWT.py}
implements the forward and backward (2D-)DWT for color images. $L$ and
$H$ stands for \emph{low-pass filtered} and \emph{high-pass filtered},
respectively.  More information about the implementation can be found at
\href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}.

\lstinputlisting[firstline=9, lastline=105, language=python, caption=DWT.py]{../src/DWT.py}

The multilevel DWT is a recursive use of the (1-level) DWT applied to
the $LL$ subband. For example, if the DWT is applied $k$-times to the
$LL$ subband, recursively, a decomposition of $k+1$-levels is generated. For
the sake of simplicity, we will denote the subbands $\{LH^k, HL^k,
HH^k\}$ as only $H^k$, and $LL^k$ as only $L^k$.

\subsection{Scalability provided by DWT}
Depending on the number of subbands levels (pyramid levels in the
pyramid representation) that we use, we can reconstruct a image at
different spatial scales (resolutions). For example, if an image $s_1$ has been transformed using the DWT of 2 levels, we get:
\begin{itemize}
\item Subband $s_1.L^2$ with the scale 2.
\item Subband $s_1.L^1=s_1.L=\text{iDWT}(s_1.L^2, S_1.H^2)$ (``iDWT'' =
  ``inverse DWT'' = ``backward DWT'') with the scale 1.
\item $s_1.L^0=\text{iDWT}(s_1.L, s_1.H)$ (the original image) with the scale 0.
\end{itemize}

\subsection{2D-DWT 1-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

# Copy a image (the output directory must be the same than the input one):
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# 2D 1-levels forward DWT:
python3 -O DWT.py -p /tmp/ -i 000

# Visualize the subbands:
display -normalize /tmp/LL000.png # <- LL1
display -normalize /tmp/LH000.png # <- LH1
display -normalize /tmp/HL000.png # <- HL1
display -normalize /tmp/HH000.png # <- HH1

# 2D 1-levels backward DWT:
python3 -O DWT.py -b -p /tmp/ -i 000

# Visualize the reconstruction:
display -normalize /tmp/000.png 

# Show diferences:
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display -normalize /tmp/diffs.png
\end{verbatim}

\subsection{2D-DWT 2-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

# Copy a image (the output directory must be the same than the input one):
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# First level:
python3 -O DWT.py -p /tmp/ -i 000

# Second level:
python3 -O DWT.py -p /tmp/LL -i 000

# Visualize the subbands:
ls -l /tmp/*.png

# Reverse second level:
python3 -O DWT.py -b -p /tmp/LL -i 000
display -normalize /tmp/LL000.png

# Reverse first level:
python3 -O DWT.py -b -p /tmp/ -i 000
display -normalize /tmp/000.png

# Show diferences:
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display -normalize /tmp/diffs.png
\end{verbatim}

%}}}

\section{Motion DWT (MDWT)}
%{{{

DWT can be applied to a sequence of images by simply transforming each
image of the sequence independently. This is done, for example, in the
\href{https://en.wikipedia.org/wiki/JPEG_2000}{motion JPEG2000 video
  compression standard}. Notice that only the spatial redundancy is
exploited. All the temporal redundancy still remains in the video.

\begin{figure}
  \centering %
  \myfig{graphics/forward_MDWT}{8cm}{800} %
  \caption{Forward MDWT step.} %
  \label{fig:forward_DWT_step}
\end{figure}

\lstinputlisting[firstline=16, lastline=62, language=python, caption=MDWT.py]{../src/MDWT.py}

\subsection{Scalability provided by MDWT}
MDWT sequences (of (subband) decompositions) are scalable in space and
in time. Spatial scalability is a direct consequence of DWT. On the other
hand, it is trivial to observe that MDWT provides \emph{fully}
temporal scalability (we can access to the images randomly) because
each image of the input sequence is transformed independently.

\subsection{2D-MDWT 1-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

# Create the output directory for the 1st-level decompositions:
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/ -N 5

# Visualize the subbands:
for i in /tmp/LL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LL*.norm
for i in /tmp/LH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LH*.norm
for i in /tmp/HL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HL*.norm
for i in /tmp/HH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HH*.norm

# 2D 1-levels backward MDWT:
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction:
rm /tmp/*.norm
for i in /tmp/00?.png; do convert -normalize $i $i.norm; done; animate /tmp/00?.png.norm
\end{verbatim}

\subsection{2D-MDWT 2-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

# Create the output directory for the 1st-level decompositions:
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 2-levels forward MDWT:
python3 -O MDWT.py -p /tmp/ -N 5
python3 -O MDWT.py -p /tmp/LL -N 5


# 2D 2-levels backward MDWT:
python3 -O MDWT.py -b -p /tmp/LL -N 5
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction:
rm /tmp/*.norm
for i in /tmp/00?.png; do convert -normalize $i $i.norm; done; animate /tmp/00?.png.norm
\end{verbatim}

%}}}

\section{Video transform alternatives}
%{{{

To remove both, spatial and temporal redundancies, two different
alternatives are available: (1) t+2D transforms and (2) 2D+t
transforms. In a t+2D transform, the video is first
\href{https://en.wikipedia.org/wiki/Digital_filter\#Analysis_techniques}{analyzed}
(transformed) over the time domain and next, over the space domain. A
2D+t transform does just the opposite. Each choice has a number of
\emph{pros} and \emph{cons}. For example, in a t+2D transform we can
apply directly any image predictor based on
\href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} because the input is a normal video. However, if we
implement a 2D+t transform, the input to the motion estimator is a
sequence decompositions.
\href{http://www.polyvalens.com/blog/wavelets/theory}{The overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means $\text{DWT}(s[t]) \neq
\text{DWT}(s[t+x])$, where $x$ is a displacement of $s[t]$ along one of the
signal domains.  Therefore, motion estimators which compare pixel values
will not work on the (subband) decomposition domain. On the other
hand, if we want to provide true spatial scalability (processing only
those
\href{https://www.tutorialspoint.com/dip/spatial_resolution.htm}{spatial
  resolutions} (scales) necessary to get a spatially scaled of our
video), a t+2D transformed video presents some drawbacks:

\begin{itemize}
\item In order to implement a lossless system, the t stage must be
  carried out at full (scale 0) resolution.
\item We can restrict the perfect reconstruction only to the scale 0
  case, but in this case, a spatial resolution representation of the
  motion information should be available at the decoder. This can be
  done, but in general, scalabilitity decreases the compression ratio
  (on the motion data in this case).
\item We can use the full (scale 0) resolution version of the motion
  information for reconstructing all the scales of the texture
  (interpolating it for all those scales greater that 0), but in this
  case the memory and computing requirements at the decoders will
  increase. Therefore, depending on the resolution of the scale 0 and the
  computational resources, this could be affordable or not.
\item As an alternative to this last problem, we can decrease the
  accuracy of the motion information in order to make it suitable for
  the spatial resolution. However, this will increase the distortion
  of the reconstruction compared to the previous solution.
\end{itemize}

Finally, it is important to realize that the presence of the motion
data in the code-stream introduces also complexity into the decoding
process because, in a quality scalable scenario, it is not trivial
(specially when non-linear systems such as those based on ME are
involved) to decide how to interleave the motion and the texture
information in a code-stream that can be decoded following different
orders, depending on the type of used scalability and the avaliable
bandwidth.

%}}}

% Subband Motion Compensated Temporal Filtering (SMCTF) 
\section{Motion Compensated Discrete Wavelet Transform (MCDWT)}
%{{{

MCDWT is a 2D+t transform. The 2D stage is MDWT. The t stage is a
1D-DWT, which removes the temporal redundancy between adjacent $H$
subbands by means of
\href{https://en.wikipedia.org/wiki/Motion_compensation}{Motion
  Compensation (MC)}. A special characteristic of MCDWT is that the
motion information is not transmitted from the
\href{https://en.wikipedia.org/wiki/Encoder}{encoder} to the
\href{https://en.wikipedia.org/wiki/Decoder}{decoder}. This is
possible because to estimate de motion at the encoder, only the
information that it is accesible by the decoder is used.

\subsection{Butterfly}

MCDWT butterly inputs three decompositions $a=\{a.L, a.H\}$, $b=\{b.L,
b.H\}$ and $c=\{c.L, c.H\}$, and outputs a residue subband
$\tilde{b}.H$, which replaces to $b.H$ in the original $b$
decomposition. So, after the use of the bufferfly, we get $a=\{a.L,
a.H\}$ (an
\href{https://en.wikipedia.org/wiki/Video_compression_picture_types}{intra-coded
  ``I'' image}), $\tilde{b}=\{b.L, \tilde{b}.H\}$ (a
\href{https://en.wikipedia.org/wiki/Video_compression_picture_types}{bidirectionally
  predicted ``B'' image}) and $c=\{c.L, c.H\}$ (another intra-coded
``I'' image). This replacement is fully reversible because the forward
transform will use only the information that the backward transform
has access to. Notice that the
\href{http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf}{pyramid
  domain} (which is invariant to the pixels displacements) has been
used to estimate and compensate the $H$ subbands.

\begin{figure}
  \centering %
  \myfig{graphics/forward_butterfly}{12cm}{1200} %
  \caption{Forward MCDWT butterfly.} %
  \label{fig:forward_butterfly}
\end{figure}
\lstinputlisting[firstline=24, lastline=57, language=python, caption={MCDWT.py (extract)}]{../src/MCDWT.py}
\lstinputlisting[firstline=4, lastline=9, language=python, caption={simple\_average.py (extract)}]{../src/simple_average.py}

Notice that, even if $a$ and $c$ (at full resolution) are available at
the decoder, only $b.L$ is. So, it does not make sense to search the
low resolution structures of $b.L$ in the high resolution images $a$
and $c$, because even if a perfect match is achieved (think about the
three decompositions $a$, $b$ and $c$ are identical), the prediction error
would be different than $0$ as a consequence of that the high
resolution information is missing in $b.L$.

\subsection{Forward and backward (inverse) transform}

The MCDWT forward transform is the result of appliying the MCDWT
butterfly to the full input sequence of decompositions with different
distances between them. As a result, except for the first and the last
image of the sequence, all the $H$-subbands are temporally
decorrelated.

\begin{figure}
  \centering %
  \myfig{graphics/temporal_decorrelation}{12cm}{1200} %
  \caption{MCDWT forward transform.} %
  \label{fig:forward_MCDWT}
\end{figure}

\lstinputlisting[firstline=17, lastline=76, language=python, caption={Code of the forward transform (MCDWT.py)}]{../src/MCDWT.py}
\lstinputlisting[firstline=78, lastline=90, language=python, caption={Code of the backward transform (MCDWT.py)}]{../src/MCDWT.py}

\subsection{(Spatial) Multiresolution}

\begin{figure}
  \centering %
  \myfig{graphics/multiresolution}{12cm}{1200} %
  \caption{MDWT+MCDWT multiresolution procedure. The input to the
    second iteration of the MDWT+MCDWT transform is the output of a
    previous (first) iteration MDWT+MCDWT transform. The second
    iteration is only applied to the scale 1.} %
  \label{fig:multiresolution}
\end{figure}

Spatial dyadic multiresolution can be obtained by appliying a sequence
of MDWT+MCDWT steps (see Fig.~\ref{fig:multiresolution}).

\subsection{Scalability provided by MCDWT}
The decomposition generated by the MCDWT is identical to
MDWT. Therefore, MCDWT provides the same spatial scalability than
MDWT. In order to obtain several spatial resolutions, MCDWT should be
applied to the low-frequency subbands, recursively. As an example, in
the Figure~\ref{fig:multiresolution}, MCDWT has been applied to the
output of the example of the Figure~\ref{fig:forward_MCDWT}. Notice
that between two consecutive iterations of MCDWT, MDWT must be applied
to the low-frequency subband in order to reduce the resolution of the
analysis.

On the other hand, MCDWT decreases the temporal scalability offered by
MDWT to allowing only dyadic access (depending on required image and
the number of MCDWT levels, more than one decomposition needs to be
inversely transformed). For example, if only one step of the backward
transform is applied to the example of the
Figure~\ref{fig:forward_MCDWT}, only the decompositions of $s_0$,
$s_2$ and $s_4$ will be reconstructed (the second
\href{https://en.wikipedia.org/wiki/Temporal_resolution}{temporal
  resolution}). If a second iteration of the backward transform is
used, all the decompositions are rendered.

\subsection{MCDWT 1-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/*.png /tmp

# 1D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/

# 1D 1-levels forward MCDWT:
python3 -O MCDWT.py -p /tmp/

rm /tmp/00?.png

# 1D 1-levels backward MCDWT:
python3 -O MCDWT.py -b -p /tmp/

# 1D 1-levels backward MDWT:
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction:
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/???.png.png
\end{verbatim}

\subsection{Showing some statistics}
\begin{verbatim}
# You must be in the 'src' directory.

cp ../sequences/stockholm/*.png /tmp

# 1D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/

# First and last decompositions are not transformed by MCDWT (step 1/2):
ls -l /tmp/HH000.png
ls -l /tmp/HL004.png

# The rest, are transformed (step 1/2):
ls -l /tmp/HL001.png
ls -l /tmp/LH002.png
ls -l /tmp/HH003.png

# Showing statistics (step 1/2):
python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/show_statistics.py -i /tmp/HL001.png
python3 ../tools/show_statistics.py -i /tmp/HH001.png
python3 ../tools/show_statistics.py -i /tmp/LH002.png
python3 ../tools/show_statistics.py -i /tmp/HL002.png
python3 ../tools/show_statistics.py -i /tmp/HH002.png

# 1D 1-levels forward MCDWT:
python3 -O MCDWT.py -p /tmp/

# First and last decompositions are not transformed by MCDWT step 2/2):
ls -l /tmp/HH000.png
ls -l /tmp/HL004.png

# The rest, are transformed (step 2/2):
ls -l /tmp/HL001.png
ls -l /tmp/LH002.png
ls -l /tmp/HG003.png

# Showing statistics:
python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/show_statistics.py -i /tmp/HL001.png
python3 ../tools/show_statistics.py -i /tmp/HH001.png
python3 ../tools/show_statistics.py -i /tmp/LH002.png
python3 ../tools/show_statistics.py -i /tmp/HL002.png
python3 ../tools/show_statistics.py -i /tmp/HH002.png

# Visualization of the H-subbands
for i in /tmp/LH00?.png; do convert -normalize $i $i.png; done; animate /tmp/LH00?.png.png
for i in /tmp/HL00?.png; do convert -normalize $i $i.png; done; animate /tmp/HL00?.png.png
for i in /tmp/HH00?.png; do convert -normalize $i $i.png; done; animate /tmp/HH00?.png.png
\end{verbatim}

\subsection{Filtering the MC subbands}
\begin{verbatim}
# You must be in the 'src' directory.

cp ../sequences/stockholm/*.png /tmp

# 1D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/

# 1D 1-levels forward MCDWT:
python3 -O MCDWT.py -p /tmp/

# Let's clear the motion compensated subbands and reconstruct the sequence ...

# Create a zero subband:
bash ../tools/create_black_image.sh; python ../tools/add_offset.py -i /tmp/zero.png -o /tmp/zero16.png -f 32768
python3 ../tools/show_statistics.py -i /tmp/zero16.png

# "Delete" the motion compensated subbands:
yes | cp /tmp/zero16.png /tmp/LH001.png
yes | cp /tmp/zero16.png /tmp/HL001.png
yes | cp /tmp/zero16.png /tmp/HH001.png
yes | cp /tmp/zero16.png /tmp/LH002.png
yes | cp /tmp/zero16.png /tmp/HL002.png
yes | cp /tmp/zero16.png /tmp/HH002.png
yes | cp /tmp/zero16.png /tmp/LH003.png
yes | cp /tmp/zero16.png /tmp/HL003.png
yes | cp /tmp/zero16.png /tmp/HH003.png

# 1D 1-levels backward MCDWT:
python3 -O MCDWT.py -b -p /tmp/

# 1D 1-levels backward MDWT:
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction:
for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

# Visualization of the differences with the original sequence:
rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

\subsection{2D-MDWT-MCDWT 2-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -p /tmp/
python3 -O MDWT.py -p /tmp/LL
python3 -O MCDWT.py -p /tmp/LL

rm /tmp/00?.png

python3 -O MCDWT.py -p /tmp/LL -b
python3 -O MDWT.py -p /tmp/LL -b
python3 -O MCDWT.py -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

%}}}

\section{Adaptive motion compensation based on the distortion of the prediction error}
%{{{

As can be seen in the MCDWT bufferfly (see
Fig.~\ref{fig:forward_butterfly}), both predictions $[b_a.H]$ and
$[b_c.H]$ have the same weight to build the prediction
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H] + [b_c.H]}{2}.
\end{equation}
This simple computation, that can have very effective for example in
\href{https://biteable.com/blog/tips/video-transitions-effects-examples/}{dissolves}
video transitions, can also be counterproductive when objects appear
only in one of the reference images. For this reason, in this section
a different predictor is proposed, based on the prediction error
generated during the ME process.

Let $[b_a.L]$ the prediction generated for the subband $[b.L]$ using
$[a.L]$ as reference and $[a.L]\rightarrow [b.L]$ as motion, and let
$[b_c.L]$ the prediction generated for $[b.L]$ using $[c.L]$ as
reference and $[c.L]\rightarrow [b.L]$ as motion. We now compute the
prediction errors
\begin{equation}
  \begin{array}{l}
    {[e_a.L]} = [b.L] - [b_a.L]\\
    {[e_c.L]} = [b.L] - [b_c.L].
  \end{array}
\end{equation}

Now, we define the similarity matrices as
\begin{equation}
  \begin{array}{l}
    {[s_a.L]} = \frac{1}{1+{|[e_a.L]|}}\\
    {[s_c.L]} = \frac{1}{1+{|[e_c.L]|}}.    
  \end{array}
  \label{eq:weighted_prediction}
\end{equation}
Notice that, if (for example) the error $[e_a.L]_{x,y}=0$, the
similarity is $[s_a.L]_{x,y}=1$ (the maximum similarity), and if the
error is high, the similarity tends to be $0$.

With this information, that can be recovered by the decoder, the
improved prediction is defined as
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H][s_a.L]+[s_c.L][b_c.H]}{[s_a.L]+[s_c.L]}.
\end{equation}

Notice that, if (for example) $[s_a.L]_{x,y}=1$ and
$[s_c.L]_{x,y}\approx 0$, then
$[\hat{b.H}]_{x,y}\approx [b_a.H]_{x,y}$, and viceversa. If
$[s_a.L]_{x,y}=[s_c.L]_{x,y}$, then
$[\hat{b.H}]_{x,y}=\frac{[b_a.H]+[b_c.H]}{2}$ (even if both similarities are small).

\lstinputlisting[firstline=4, lastline=15, language=python, caption={weighted\_average.py (extract)}]{../src/weighted_average.py}

%}}}

\section{Orthogonal, orthonormal, and biorthogonal transforms}
%{{{

In signal processing\footnote{In mathemathics the definition of
  orthogonality refers to characteristics such as the basis of the
  transform forms an orthogonal espace, where it is impossible to
  represent one of the basis as a linear combination of the rest of
  basis (or in other words, if the inner product is zero).}, a
transform (such as the Discrete Cosine Transform, the Walsh-Hadamard
Transform or the Karhunen-Loeve Transform) is orthogonal when the
coefficients generated by the transform are uncorrelated (there is no
way to infer one coefficient from another).

If the norm of all the basis of an orthogonal transform is one,
then the transform is said orthonormal. Orthonormal transforms are
interesting because:
\begin{enumerate}
\item They are energy preserving: the energy of the output is the same
  than the energy of the input. This means that, for example, a
  quantization error produced in a coefficient of the transform will
  generate the same quantization error at the output (the complete
  signal) of the inverse transform. The same holds by the forward
  transform.
\item The transform matrix of the inverse transform is the transpose
  of the forward transform. In orthogonal transforms, the transform
  matrix of the inverse transform is the inverse of the transform
  matrix of the forward transform.
\end{enumerate}

Biorthogonal transforms do no satisfy any of these features: they are
not energy preserving (this can be also observed because the
frequency-domain responses of the analysis and synthesis filters are
not symmetric), and there is not an algebraic way (matrix
transposition/inversion) to compute the backward transform from the
forward one, and viceversa. This, that can be considered as a
drawback, gives an extra degree of freedom to design the analysis and
the synthesis filters (whose only requirement is that transform pair
to be reversible), providing in general the possibility of using more
sofisticated filters such as those based on non-linear filtering, as
for example, those based on motion estimation algorithms.

In general, each subband $b$ of a decomposition generated by a
biorthogonal 2D-DWT transform have a different subband gain
$\alpha_b$. Usually, the lower the frequency of the subband, the
higher the gain. Notice also, that these gains also are different for
each different transform.

Subband gains are important in lossy signal compression because they
quantify the relative importance of the wavelet coefficients of the
different subbands when we introduce distortion in the wavelet
domain. Thus, for example, if we decide to quantize a wavelet
coefficient, the amount of distortion that we are generating in the
signal domain will depend on the subband where that coefficient is
localized. In general, low-frequency coefficients are more
``important'' that high-frequency ones.

To compute the subband gains we have two options:
\begin{enumerate}
\item The algebraic way. We will need the expressions of the four
  filters (two anaylsis filters and two synthesis filters) and deduce
  the gains.
\item The algorithmic way. We will need to compute the energy of the
  impulse response of the inverse transform, when we apply such
  impulse to each one of the subbands of the decomposition. Supposing
  that, after appliying the DWT to an image, the coefficients of the
  subband HH are the least energetic with an energy $x$, the subband
  gain for subband $b$ is computed as
  \begin{equation}
    \alpha_b = \epsilon_b/x,
  \end{equation}
  where $\epsilon_b$ is the energy of the reconstruction when the
  impulse signal is localized at $b$. Notice that all the gains should
  be larger than one.
\end{enumerate}

This computation can be also applied to MCDWT, by computing the subband
gains as a function of the number $T$ of temporal decompositions.

%}}}

\section{Quantization effects}
%{{{
\subsection{Quantization of ($H$) MC subbands}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/ -N 3
python3 -O MCDWT.py -p /tmp/ -N 3

q_step=32
python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 -O MCDWT.py -p /tmp/ -b -N 3
python3 -O MDWT.py -p /tmp/ -b -N 3

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

\subsection{Quantization of all $H$ subbands}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/ -N 3
python3 -O MCDWT.py -p /tmp/ -N 3

q_step=32
python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 -O MCDWT.py -p /tmp/ -b -N 3
python3 -O MDWT.py -p /tmp/ -b -N 3

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

% bash ../tools/create_black_image.sh; python ../tools/add_offset.py -i /tmp/zero.png -o /tmp/zero16.png -f 32768
% yes | cp /tmp/zero16.png /tmp/LH000.png
% yes | cp /tmp/zero16.png /tmp/HL000.png
% yes | cp /tmp/zero16.png /tmp/HH000.png
% yes | cp /tmp/zero16.png /tmp/LH001.png
% yes | cp /tmp/zero16.png /tmp/HL001.png
% yes | cp /tmp/zero16.png /tmp/HH001.png
% yes | cp /tmp/zero16.png /tmp/LH002.png
% yes | cp /tmp/zero16.png /tmp/HL002.png
% yes | cp /tmp/zero16.png /tmp/HH002.png

\subsection{Quantization of all $H$ subbands}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/ -N 3
python3 -O MCDWT.py -p /tmp/ -N 3

q_step=128
python3 ../tools/quantize.py -i /tmp/LL000.png -o /tmp/LL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LL001.png -o /tmp/LL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LL002.png -o /tmp/LL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 -O MCDWT.py -p /tmp/ -b -N 3
python3 -O MDWT.py -p /tmp/ -b -N 3

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

%}}}

\section{Lossy compression effects}
%{{{

\subsection{Compression of ($H$) MC subbands}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/ -N 3
python3 -O MCDWT.py -p /tmp/ -N 3

quality=50
#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 -O MCDWT.py -p /tmp/ -b -N 3
python3 -O MDWT.py -p /tmp/ -b -N 3

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

\subsection{Quantization of all $H$ subbands}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/ -N 3
python3 -O MCDWT.py -p /tmp/ -N 3

quality=10
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640
python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 -O MCDWT.py -p /tmp/ -b -N 3
python3 -O MDWT.py -p /tmp/ -b -N 3

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

\subsection{Quantization of all $H$ subbands}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/ -N 3
python3 -O MCDWT.py -p /tmp/ -N 3

quality=10
python3 ../tools/compress.py -i /tmp/LL000.png -o /tmp/LL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL000.png -o /tmp/LL000.png -f 32640
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640
python3 ../tools/compress.py -i /tmp/LL001.png -o /tmp/LL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL001.png -o /tmp/LL001.png -f 32640
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640
python3 ../tools/compress.py -i /tmp/LL002.png -o /tmp/LL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL002.png -o /tmp/LL002.png -f 32640
python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 -O MCDWT.py -p /tmp/ -b -N 3
python3 -O MDWT.py -p /tmp/ -b -N 3

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

%}}}

\section{MCDWT decomposition}
%{{{

\begin{figure}
  \centering %
  \myfig{graphics/MCDWT_decomposition}{12cm}{1200} %
  \caption{MCDWT(1,3) decomposition ($\text{GOP\_size}=8$).} %
  \label{fig:MCDWT_decomposition}
\end{figure}

%}}}

\section{Encoding of B-type subbands}
%{{{

The subband $\tilde{b}.H$ is no longer needed for applying the
butterfly to different decompositions and scales. For this reason, we
can compress this subband. As a result, after using the butterfly to
all the decompositions of the sequence and all the scales, all the $H$
subbands will be compressed, excepting the intra-coded image of each
GOP. Notice that the redundancy exploited in B-type $H$ subbands is
temporal.

A $\tilde{b}.H$ subband can be compressed by sorting its coefficients
by energy and predicting them. Thus, the better the prediction the
higher the compression ratio. To sort the coefficients of subband
$\tilde{b}.H$ by energy without using it (remember that the decoder
does not know this information), we can think that there is a
correlation between the coefficients of $\tilde{b}.H$ and
$\tilde{b}.L$, that is, the prediction error resulting of substracting
to $b.L$ a prediction $\hat{b}.L$ generated with the same motion
information used to built the prediction $\hat{b}.H$. Such idea has
been implemented in the following algorithm:
\begin{enumerate}
\item [1.] Compute the prediction error for the $[b.L]$ subband
\begin{equation}
  [\tilde{b}.L] = [b.L] - [\hat{b}.L]
\end{equation}
where
\begin{equation}
  [\hat{b}.L] = \frac{[b_a.L][s_a.L]+[s_c.L][b_c.L]}{[s_a.L]+[s_c.L]}.
\end{equation}

\item [2.] Compute the 2D-DWT of subband $L$
  \begin{equation}
    \tilde{b}=\text{DWT}([\tilde{b.L}]).
  \end{equation}

\item [3.] Find the indices for $\tilde{b.L}$ that sorts it in descending order by energy
  \begin{equation}
    \text{indices}=\text{unravel\_index}(\text{argsort}(\text{abs}(\tilde{b.L}),
    b.L.\text{width}, b.L.\text{height})).
  \end{equation}
%\item [3.] For each coefficient $x,y$ in $\tilde{b}.L$:
%  \begin{enumerate}
%  \item [a.] $d_{i=x\times b.\text{width}+y} = (\text{abs}(\tilde{b}_{x,y}), x, y)$
%  \end{enumerate}
%\item [4.] Sort $d$ in desceding order sorted by field 0 /* use the amplitude of luminance */.
\item Go over $\tilde{b}.L$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy.
%\item [5.] Estimate the nearest power of two smaller or equan than the maximum amplitude
%  \begin{equation}
%    \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(d_{0}))).
%  \end{equation}
%\item [6.] For each coefficient $i$ in $d$:
%  \begin{enumerate}
%  \item [a.] $x,y = d_i[1:2]$.
%  \item [b.] If any of
%    $1<<\text{int}(\log_2(\tilde{b.H}_{x,y}))==\lambda_0$ then send
%    $\text{sign}(\tilde{b.H}_{x,y})$ for the three (LH, HL and HH)
%    subbands.
%  \item [b.] Send $(\hat{b.H}_{x,y}~\text{bitwise-and}~\lambda)/\lambda$ for
%    the three (LH, HL and HH) subbands.
%  \end{enumerate}
\item [7.] $\lambda >>= 1$.
\item [8.] Goto step 6.
\end{enumerate}

%}}}

\section{Encoding of I-type images}
%{{{

After a $T$-levels MCDWT transform of a sequence, $T+1$ temporal
subbands are generated, and the temporal subband $L^{T+1}$ has a one
$H$ I-type subband for each GOP. Notice that the redundancy exploited
to accumulate the visual information in the $L$ subband (or to remove
the visual information in the $H$ subbands is spatial) in a I-type
image.

After using MCDWT, I-type images are decomposed into
the four subbands $LL$ (low-frequencies), $LH$, $HL$ and $HH$
(high-frequencies). Depending on the image content and wavelets used
in the 2D-DWT, $H$ subbands contain a certain amount of \emph{visual
  information}\footnote{Any visual stimulus that provides information
  to humans}. The key here is to reduce as much as possible such
visual information, leaving in the $H$ subbands only \emph{visual
  noise}\footnote{Any visual stimulus that does not provides
  information to humans.} If we achieve this, the $H$ subbands,
expressed in a sign/magnitude representation will have two features:
(1) the probability of finding zeros when we are moving from the least
significant bit-planes fo the most significant ones of the magnitude
of the coefficients will increase (to the point where from one
bit-plane upwards, all the bits will be zero), and (2) the correlation
in the sign bit-plane will be zero. In this situation, an efficient
encoding method is to compress the bit-planes, starting at the MSBP
with at least one bit to 1 (in other words, ``send'' the ones of the
MSBP), sending before the corresponding sign. If more than one one is
found in the MSBP, the bits should be processed by descending
magnitude, i.e., sending first those bits that corresponds with
coefficients with a higher magnitude. This information can be
extracted from the $L$ subband as below is shown.

Information in the $L$ subband can be used to futher reduce the visual
information of $H$ subbands and to estimate the value of its
high-frequency coefficients, and therefore to sort them (at least
approximately) by their magnitude. The idea is to predict the $H$
coefficients by introducing some (visual) information in the $[L]$
subband, and to perform again the 2D-DWT (using the same
wavelets). This generates a $\hat{H}$ subbands that substracted to $H$
should remove the visual information from them (decorrelating the
signs and reducing the number of bits necessary to represent most of
the coefficients).
\begin{equation}
  \tilde{H} = H - \hat{H}
\end{equation}

$\tilde{H}$ is as a prediction error that basically should store
noise. Therefore, if $\tilde{H}$ is not transmitted at all, a good
approximation of $L^0$ should be recontructed. However, in most of
cases, some amount of (unpredictable) visual information will remain
in $\tilde{H}$, concentrated in the MSBPs. The current representation
of the rest of BPs (with unpredictable bits) should be
efficient. Sumarizing, $\tilde{H}$ is a triple subbands of spatially
uncorrelated coefficients, most of them small, with zeros in the MSBPs
and random bits in the LSBPs, and to progressively encode them, a BP
compressor can be used.

At this point, any BLIC (Bi-Level Image Compressor) should be
applicable. However, there is a dependencies between the BPs that
using a BLIC that cannot be exploited. The question is, could it be
possible to predict the coordinates of the ones in a BP? (by default
the BPs are zero). If this is possible, a sequence of bits (with so
many bits as coefficients are in $H$) can encode the success of such
prediction. Suppose that $\hat{H}\downarrow$ (the list of indexes the
sorts $\hat{H}$ in descending order by amplitude) determines the
possible localization of the most energetic coefficients in
$\tilde{H}$. If so, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ (the list of residue coefficients
(approximately-) sorted by energy) will be 1 (with a high probability)
and the rest will be 0 (with a high probability). For example, if
there are five 1's in the MSB of the beginning of
$\tilde{H}[\hat{H}\downarrow]$ (that is, a perfect prediction in which
we can localize the position of the most energetic coefficients, but
not its magnitude that is, if their MSB is 1 or 0), the complete BP
can be encoded with the symbols <5><0> (five 1's localized in the
first five positions of $\tilde{H}[\hat{H}\downarrow]$). If for
example, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ are 0101001000110... (obviously this
prediction is not perfect), then the output symbols are
<0><1><1><1><2><1><3><2><0>, which can be re-encoded as
<0><1,3><2,1><1,1><3,1><2,1>,<0>.

The output of the last stage is a sequence of symbols with a
exponiental probability distribution that can be encoded efficiently
with a variable-length code. Or using DPCM?

The perfect prediction of the 2nd MSBP of
$\tilde{H}[\hat{H}\downarrow]$ if formed by the first refinements bits
of those coeffientes that already are significative, and by the first
1 of those coefficients that start to being significative in this 2nd
MSBP. Such perfect prediction has the structure
1.(5).10.(7).01.(10).10...0. We can realize now that the first two
runs has a length equal to the number of coefficients significative in
the previous BP, so, we only need to encode the first run of 1's,
generating the symbols <5><0><10><0>. Let's suppose an unperfect
prediction like 1101100|0111100110101100001010...., and that the
number of significant coefficients is 7. In this case, the output is
<2><1><2><0><0><4><2><2><1><1><1><2><4><1><1><1><1><0>.

In the extreme case where the prediction is completely wrong, we are
going to generate a secuence of $N$ bits 0101010101... which can be
encoded as <0><1><1>...<1> ($N$ symbols), which can be re-encoded as
<0><1,N-1>.

%By definition, unpredictable information can not be
%compressed. Consecuently, the lossless encoding of $H$ is basically:
%(1) determine the

The coefficients of $\hat{H}$ sorted by their magnitude in descending
order can be used to send first those bits that belong to the largest
coefficients of $H$.

Obviously, the prediction
\begin{equation}
  \text{sort}(H)==\text{sort}(\hat{H})
\end{equation}
will not always be perfect. This can have two different consequences: (1) that 

Introduced such ideas, to compress the $H$ subbands of a 1-level
2D-DWT, the following algorithm can be used:

\begin{enumerate}

\item Compute $[L]$, zooming-in the $L$ subband.
  \begin{equation}
    [L] = \text{iDWT}(L, 0)
  \end{equation}
  
\item Add to $[L]$ some visual information $V$ that could be present
  in original $L^0$. Any edge enhancement algorithm should work.
  \begin{equation}
    [L'] = [L] + V
  \end{equation}

\item Compute the 2D-DWT of $[L']$ to obtain a prediction $\hat{H}$.
  \begin{equation}
    \_, \hat{H} = \text{DWT}([L'])
  \end{equation}
  Notice that $\_\approx L$.

\item Substract the prediction to the $H$ subbands.
  \begin{equation}
    \tilde{H} = H-\hat{H}
  \end{equation}

\item Sort $\hat{H}$ by magnitude in descending order. For most
  images, $\hat{H}\approx H$ and therefore, the most significant
  coefficients in both structures should be placed in the same
  coordinates.
  \begin{equation}
    \hat{H}\downarrow = \text{unravel\_index}(\text{argsort}(\text{abs}(\hat{H})))
  \end{equation}

\item Go over $\hat{H}\downarrow$, sending by bit-planes the
  sign-magnitude representation of the coefficients of $H$.

  \begin{enumerate}

  \item Estimate the nearest power of two, smaller or equal than the
    coefficient of $H$ with the maximum amplitude.
    \begin{equation}
      \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(H_{\hat{H}\downarrow[0]}))).
    \end{equation}
    $\lambda$ should be the index of the MSBP in $H$.
    
  \item Send the BP of index $\lambda$.
    \begin{equation}
      \begin{array}{l}
        \text{for~}H_{s,x,y}\text{~in} H[\hat{H}\downarrow]: \\
        ~ if \lambda > int(\log_2(abs(H_{s,x,y}))) > \lambda/2: \\
        ~~ send(sign(H_{s,x,y}))\\
        ~ 
        %~ 
        %\text{for~}d\text{~in range}(3): \\
        %~ \text{for~}y\text{~in range}(L^0.\text{height}): \\
        %~~ \text{for~}x\text{~in range}(L^0.\text{width}): \\
        %~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
        %~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
      \end{array}
    \end{equation}
  \end{enumerate}
\end{enumerate}
  
\begin{comment}
To compress the $a.H$ subband in the last iteration of the butterfly,
the following algorithm can be used:
\begin{enumerate}
\item [1.] Compute the 2D-DWT of the $a.L$ subband
\begin{equation}
  LL^2, LH^2, HL^2, HH^2 = \text{DWT}(LL). 
\end{equation}

\item [2.] Desplace 2 bits to the left the $H$ coefficients to create
  space for encoding the subband
  \begin{equation}
    \begin{array}{l}
      LH^2 <<= 2 \\
      HL^2 <<= 2 \\
      HH^2 <<= 2.
    \end{array}
  \end{equation}
\item [3.] Label the coefficients of the subbands:
  \begin{equation}
    \begin{array}{l}
      HL^2 += 1 \\
      HH^2 += 2.
    \end{array}
  \end{equation}
\item [4.] Create an array with the 3 matrices
  \begin{equation}
    H = [LH^2, HL^2, HH^2].
  \end{equation}
\item [5.] Create a linear array with the 3 flattened matrices
  \begin{equation}
    H'=\text{ravel}(H).
  \end{equation}
\item [6.] Find the indices for the $LH^2$, $HL^2$ and $HH^2$ matrices
  that sort $H'$ in descending order by energy
  ($\text{width}=LH^2.\text{shape}[0]$ and $\text{height}=LH^2.\text{shape}[1]$)
  \begin{equation}
    \text{indices} =
    \text{unravel\_index}(\text{argsort}(\text{abs}(H')), \text{width},
    \text{height}).
  \end{equation}
\item [7.] Go over $H'$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy
  \begin{equation}
    \begin{array}{l}
      \text{for~}d\text{~in range}(3): \\
      ~ \text{for~}y\text{~in range}(\text{height}): \\
      ~~ \text{for~}x\text{~in range}(\text{width}): \\
      ~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
      ~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
    \end{array}
  \end{equation}
\end{enumerate}
\end{comment}

%}}}

\begin{comment}
%{{{

\section{Progressive reconstruction}
The forward bufferfly should reduce the energy of subband $b.H$$ after
substracting a prediction (see Eq.~\ref{eq:weighted_prediction}) which
is generated using the subbands $a.H$ and $c.H$ as references. If the
prediction is good, the energy of $\tilde{b.H}$ will be small and
viceversa. If the prediction is good, the quantization of the subbands
$a.H$, $b.H$ and $c.H$ should leave more energy in the subbands $a.H$
and $c.H$ than in $b.H$. Therefore, if the prediction is good, the
progressive reconstruction of the subbands (using a progressively
small quantization step) should reconstruct first the information of
$a.H$ and $c.H$.

\section{Coefficients encoding}
Subband $\tilde{b.H}$ is not used any more as a reference and
therefore, it can be compressed. The compression algorithm sort the
coefficients of $\tilde{b.H}$ by the amplitude of the coefficients of
$\tilde{b.L}$, considering that the prediction error will affect in
the same way the low and the high frequencies of $b$. Then, the sorted
coefficients are DPCM encoded, quantized, and entropy encoded.

\section{Multilevel MCLT}
The distance between images $a$, $b$ and $c$ should decrease the
efficiency of the predictions, generating more energy in the
low-frequency subbands. Therefore, the quantization when $T>1$ should
transmit more information of the low-frequency subbands than of the
high-frequency ones when the subbands are quantized.

%}}}
\end{comment}
